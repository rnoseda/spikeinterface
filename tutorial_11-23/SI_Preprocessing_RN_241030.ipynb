{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.2 - Adapted by Rodrigo Noseda - October 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recording and Paths <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "#Pasted directly from explorer \"C:\\Users\\rodri\\Documents\\Bonsai-RN\\Bonsai_DataRN\\2023_3_21\\\"\n",
    "\n",
    "recording_paths_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.startswith('RawEphysData') and filename.endswith('.bin'):\n",
    "        recording_paths_list.append(data_folder / filename)\n",
    "print('Recording Files List:')\n",
    "print(recording_paths_list)\n",
    "\n",
    "# parameters associated to the recording in bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "rec_dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 2 #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and append recording segments to Baserecording object\n",
    "recordings_list = []\n",
    "rec = si.read_binary(recording_paths_list, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=False)\n",
    "recordings_list.append(rec)#Appends all extracted rec to a list. Kilosort does not support segments. Use concatenation.\n",
    "recording = si.concatenate_recordings(recordings_list)#Creates Object ConcatenateSegmentRecording\n",
    "#Filtering recording\n",
    "recording_f = si.bandpass_filter(recording, freq_min=300, freq_max=6000)\n",
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "recording_layers = dict(raw=recording,\n",
    "                        filt=recording_f, \n",
    "                        common=recording_cmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = si.plot_traces(recording_layers, time_range=[569, 600], channel_ids=[45],\n",
    "                return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing: Artifact Removal <a class=\"anchor\" id=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate time difference in seconds\n",
    "def time_difference_in_seconds(start_time, end_time):\n",
    "    start = datetime.combine(datetime.min, start_time)\n",
    "    end = datetime.combine(datetime.min, end_time)\n",
    "    return (end - start).total_seconds()\n",
    "\n",
    "# Load the Timestamps CSV files, extract start time\n",
    "timestamp_start_df = pd.read_csv(data_folder / 'TimestampsEphys_0.csv', nrows=1, header=None, names=['Timestamps'])\n",
    "timestamp_start = pd.to_datetime(timestamp_start_df['Timestamps'][0]).time() # Convert timestamps to datetime objects (only time part)\n",
    "\n",
    "#Load ttl times from csv files, calculate time in seconds\n",
    "ttl_files = sorted(glob.glob(os.path.join(data_folder, \"TTL*.csv\")))\n",
    "concatenated_ttl_times = pd.DataFrame()\n",
    "for ttl_file in ttl_files:\n",
    "    TTL_df = pd.read_csv(ttl_file, header=None, names=['TTL_Times'])#(usecols=[0], nrows=1)\n",
    "    concatenated_ttl_times = pd.concat([concatenated_ttl_times, TTL_df], ignore_index=True)\n",
    "concatenated_ttl_times['TTL_Times'] = pd.to_datetime(concatenated_ttl_times['TTL_Times']).dt.time # Convert the TTL timestamps to timedelta (ignoring date)\n",
    "# Apply the time difference function to each row in the TTL DataFrame\n",
    "concatenated_ttl_times['time_diff_seconds'] = concatenated_ttl_times['TTL_Times'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "#print(concatenated_ttl_times)\n",
    "\n",
    "#Load events times from csv files, calculate time in seconds\n",
    "events_files = sorted(glob.glob(os.path.join(data_folder, \"Events*.csv\")))\n",
    "concatenated_event_times = pd.DataFrame()\n",
    "for event_file in events_files:\n",
    "    Events_df = pd.read_csv(event_file, header=None, usecols=[0, 1], names=['Stim_start', 'Stim_end'])#(usecols=[0], nrows=1)\n",
    "    concatenated_event_times = pd.concat([concatenated_event_times, Events_df], ignore_index=True)\n",
    "concatenated_event_times['Stim_start'] = pd.to_datetime(concatenated_event_times['Stim_start']).dt.time # Convert the TTL timestamps to timedelta (ignoring date)\n",
    "concatenated_event_times['Stim_end'] = pd.to_datetime(concatenated_event_times['Stim_end']).dt.time # Convert the TTL timestamps to timedelta (ignoring date)\n",
    "# Apply the time difference function to each row in the TTL DataFrame\n",
    "concatenated_event_times['time_diff_start_seconds'] = concatenated_event_times['Stim_start'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "concatenated_event_times['time_diff_end_seconds'] = concatenated_event_times['Stim_end'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "concatenated_event_times['stim_duration'] = concatenated_event_times['time_diff_end_seconds'] - concatenated_event_times['time_diff_start_seconds'] \n",
    "#print(concatenated_event_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single channel trace to store the x coordinates on key_press (for remove artifact function)\n",
    "coordinates_x = []\n",
    "start_time = 6311 * fs\n",
    "end_time = 6312 * fs\n",
    "fig, ax = plt.subplots()\n",
    "trace = recording_f.get_traces(start_frame=start_time, end_frame=end_time, channel_ids=[8, 43, 44], return_scaled=True)\n",
    "time_axis = np.arange(start_time, end_time) / fs\n",
    "ax.plot(time_axis, trace)\n",
    "\n",
    "# Function to capture click events\n",
    "def onclick(event):\n",
    "    if event.xdata is not None:\n",
    "        coordinates_x.append((event.xdata))# Store the key_press coordinates in the list\n",
    "        print(f\"Key pressed at: x={event.xdata}\")# Display the coordinates\n",
    "# Connect the key_press event to the figure\n",
    "cid = fig.canvas.mpl_connect('key_press_event', onclick)\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write initial coordinates_x csv file containing a column of artifact timestamps (from onclick function) in seconds\n",
    "def save_to_csv(filename, float_list):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for value in float_list:\n",
    "            writer.writerow([value])\n",
    "filename = 'artifacts_coordinates_xb.csv'\n",
    "save_to_csv(data_folder / filename, coordinates_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append more floats to same coordinates_x csv file\n",
    "def append_to_csv(filename, float_list):\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for value in float_list:\n",
    "            writer.writerow([value])\n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "append_to_csv(data_folder / filename, coordinates_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted list of triggers in frames from coordinates_x csv file\n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "triggers_in_sec = []\n",
    "with open(data_folder / filename, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        for item in row:\n",
    "            triggers_in_sec.append(float(item))\n",
    "triggers_in_sec = [round(triggers_in_sec, 6) for triggers_in_sec in triggers_in_sec]\n",
    "triggers_in_sec.sort()\n",
    "triggers_in_frames = [round(i * fs) for i in triggers_in_sec]\n",
    "\n",
    "#Creates list of tuples with start_frame and end_frame for artifact removal.\n",
    "def create_tuples(triggers_in_frames):\n",
    "    result = [(frame - 300, frame) for frame in triggers_in_frames] #300 frames is 10ms. Each frame represent the end of the artifact.\n",
    "    return result\n",
    "list_periods = create_tuples(triggers_in_frames)\n",
    "print(list_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted list of triggers in frames from coordinates_x csv file\n",
    "from operator import itemgetter \n",
    "filename = 'long_artifacts_coordinates_x.csv'\n",
    "noise_in_sec = []\n",
    "with open(data_folder / filename, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        for item in row:\n",
    "            noise_in_sec.append(float(item))\n",
    "noise_in_sec = [round(noise_in_sec, 6) for noise_in_sec in noise_in_sec]\n",
    "noise_in_sec.sort()\n",
    "\n",
    "a = [1, 3, 5, 7]\n",
    "noise_in_sec = (itemgetter(*a)(noise_in_sec))\n",
    "noise_in_frames = [round(i * fs) for i in noise_in_sec]\n",
    "noise1 = noise_in_frames[0:1]\n",
    "noise2 = noise_in_frames[1:2]\n",
    "noise3 = noise_in_frames[2:3]\n",
    "noise4 = noise_in_frames[3:4]\n",
    "print(noise_in_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean recording with silence_periods. List periods is one list per segment of tuples (start_frame, end_frame).\n",
    "recording_clean = si.silence_periods(recording_cmr, list_periods=list_periods, seed=0, mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_layers2 = dict(raw=recording, common=recording_cmr, clean=recording_clean) \n",
    "w = si.plot_traces(recording_layers2, time_range=[0, 60], channel_ids=[15, 16, 17, 42, 61],\n",
    "                    return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_clean.save(format='binary', folder=data_folder / \"recording_clean\", **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structured array\n",
    "#data = np.array([(1.5, 2.5, 'Hello'), (3.0, 4.0, 'World')], \n",
    "#                 dtype=[('col1', 'f8'), ('col2', 'f8'), ('col3', 'U100')])\n",
    "\n",
    "# Save the array to a .npy file\n",
    "#np.save('my_array.npy', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Si_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
