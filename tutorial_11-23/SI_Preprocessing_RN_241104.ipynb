{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.2 - Adapted by Rodrigo Noseda - November 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recording and Paths <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "\n",
    "recording_paths_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.startswith('RawEphysData') and filename.endswith('.bin'):\n",
    "        recording_paths_list.append(data_folder / filename)\n",
    "print('Recording Files List:')\n",
    "print(recording_paths_list)\n",
    "\n",
    "# parameters associated to the recording in bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "rec_dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concatenate Recording, Filtering and Probe setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and append recording segments to Baserecording object\n",
    "recordings_list = []\n",
    "rec = si.read_binary(recording_paths_list, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=False)\n",
    "recordings_list.append(rec)#Appends all extracted rec to a list. Kilosort does not support segments. Use concatenation.\n",
    "recording = si.concatenate_recordings(recordings_list)#Creates Object ConcatenateSegmentRecording\n",
    "#Filtering recording\n",
    "recording_f = si.bandpass_filter(recording, freq_min=400, freq_max=5000)\n",
    "recording_f_cmr = si.common_reference(recording_f, reference='global', operator='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probe from library and set channel mapping\n",
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "print(f\"ProbeInterface version: {pi.__version__}\")\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-158-H10' #probe_name = 'ASSY-158-F' #probe_name = 'ASSY-158-H6'\n",
    "probeH10 = pi.get_probe(manufacturer, probe_name)#library: comes with contact_ids and shank_ids info.\n",
    "\n",
    "#Mapping Intan (device) channels and setting them to probe (RHD-2132/2164)\n",
    "device_channel_indices = [24,23,25,22,26,21,27,20,28,19,29,18,30,17,31,16,0,15,1,14,2,13,3,12,4,11,5,10,6,9,7,8,\n",
    "    56,55,57,54,58,53,59,52,60,51,61,50,62,49,63,48,32,47,33,46,34,45,35,44,36,43,37,42,38,41,39,40] #Modify accordingly.\n",
    "#   88,87,89,86,90,85,91,84,92,83,93,82,94,81,95,80,64,79,65,78,66,77,67,76,68,75,69,74,70,73,71,72,\n",
    "#   120,119,121,118,122,117,123,116,124,115,125,114,126,113,127,112,96,111,97,110,98,109,99,108,100,107,101,106,102,105,103,104]\n",
    "probeH10.set_device_channel_indices(device_channel_indices)\n",
    "\n",
    "#Set and group by shank probe before sorting\n",
    "recording_f_cmr_prb = recording_f_cmr.set_probe(probeH10, group_mode=\"by_shank\")\n",
    "\n",
    "#Plotting probe\n",
    "fig, ax = plt.subplots(figsize=(6, 10))\n",
    "plot_probe(probeH10, ax=ax, with_contact_id=True, with_device_index=True)\n",
    "ax.set_xlim(-20, 210)\n",
    "ax.set_ylim(-75, 320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Timestamps, TTLs and Events \n",
    "### Converted to seconds and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate time difference in seconds\n",
    "def time_difference_in_seconds(start_time, end_time):\n",
    "    start = datetime.combine(datetime.min, start_time)\n",
    "    end = datetime.combine(datetime.min, end_time)\n",
    "    return (end - start).total_seconds()\n",
    "\n",
    "# Load the Timestamps CSV files, extract start time\n",
    "timestamp_start_df = pd.read_csv(data_folder / 'TimestampsEphys_0.csv', nrows=1, header=None, names=['Timestamps'])\n",
    "timestamp_start = pd.to_datetime(timestamp_start_df['Timestamps'][0]).time() # Convert timestamps to datetime objects (only time part)\n",
    "\n",
    "#Load times from timestamps csv files and calculate start time in seconds.\n",
    "tms_files = sorted(glob.glob(os.path.join(data_folder, \"Timestamps*.csv\")))\n",
    "concatenated_segment_times = pd.DataFrame()\n",
    "for tms_file in tms_files:\n",
    "    tms_df = pd.read_csv(tms_file, header=None)#, names=['Start_Times', 'End_Times'])#(usecols=[0], nrows=1)\n",
    "    segment_times = pd.DataFrame({'Start_Times': tms_df.iloc[0], 'End_Times': tms_df.iloc[-1]})\n",
    "    concatenated_segment_times = pd.concat([concatenated_segment_times, segment_times], ignore_index=True)\n",
    "concatenated_segment_times['Start_Times'] = pd.to_datetime(concatenated_segment_times['Start_Times']).dt.time # Convert the tms timestamps to timedelta (ignoring date)\n",
    "concatenated_segment_times['End_Times'] = pd.to_datetime(concatenated_segment_times['End_Times']).dt.time # Convert the tms timestamps to timedelta (ignoring date)\n",
    "#concatenated_segment_times['Segment_Times'] = pd.to_datetime(concatenated_segment_times['Segment_Times']).dt.time # Convert the tms timestamps to timedelta (ignoring date)\n",
    "# Apply the time difference function to each row in the tms DataFrame\n",
    "concatenated_segment_times['Start_Times_seconds'] = concatenated_segment_times['Start_Times'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "concatenated_segment_times['End_Times_seconds'] = concatenated_segment_times['End_Times'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "concatenated_segment_times['Segment_duration_seconds'] = concatenated_segment_times['End_Times_seconds'] - concatenated_segment_times['Start_Times_seconds']\n",
    "#concatenated_segment_times.to_csv(data_folder / 'concatenated_segment_times.csv', mode='x', index=False)\n",
    "#print(concatenated_segment_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ttl times from csv files, calculate time in seconds\n",
    "ttl_files = sorted(glob.glob(os.path.join(data_folder, \"TTL*.csv\")))\n",
    "concatenated_ttl_times = pd.DataFrame()\n",
    "for ttl_file in ttl_files:\n",
    "    TTL_df = pd.read_csv(ttl_file, header=None, names=['TTL_Times'])#(usecols=[0], nrows=1)\n",
    "    concatenated_ttl_times = pd.concat([concatenated_ttl_times, TTL_df], ignore_index=True)\n",
    "concatenated_ttl_times['TTL_Times'] = pd.to_datetime(concatenated_ttl_times['TTL_Times']).dt.time # Convert the TTL timestamps to timedelta (ignoring date)\n",
    "# Apply the time difference function to each row in the TTL DataFrame\n",
    "concatenated_ttl_times['time_diff_seconds'] = concatenated_ttl_times['TTL_Times'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "#concatenated_ttl_times.to_csv(data_folder / 'concatenated_ttl_times.csv', mode='x', index=False)\n",
    "#print(concatenated_ttl_times)\n",
    "\n",
    "#Load events times from csv files, calculate time in seconds\n",
    "events_files = sorted(glob.glob(os.path.join(data_folder, \"Events*.csv\")))\n",
    "concatenated_event_times = pd.DataFrame()\n",
    "for event_file in events_files:\n",
    "    Events_df = pd.read_csv(event_file, header=None, usecols=[0, 1], names=['Stim_start', 'Stim_end'])#(usecols=[0], nrows=1)\n",
    "    concatenated_event_times = pd.concat([concatenated_event_times, Events_df], ignore_index=True)\n",
    "concatenated_event_times['Stim_start'] = pd.to_datetime(concatenated_event_times['Stim_start']).dt.time # Convert the TTL timestamps to timedelta (ignoring date)\n",
    "concatenated_event_times['Stim_end'] = pd.to_datetime(concatenated_event_times['Stim_end']).dt.time # Convert the TTL timestamps to timedelta (ignoring date)\n",
    "# Apply the time difference function to each row in the TTL DataFrame\n",
    "concatenated_event_times['time_diff_start_seconds'] = concatenated_event_times['Stim_start'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "concatenated_event_times['time_diff_end_seconds'] = concatenated_event_times['Stim_end'].apply(lambda x: time_difference_in_seconds(timestamp_start, x))\n",
    "concatenated_event_times['stim_duration'] = concatenated_event_times['time_diff_end_seconds'] - concatenated_event_times['time_diff_start_seconds']\n",
    "#concatenated_event_times.to_csv(data_folder / 'concatenated_event_times.csv', mode='x', index=False) \n",
    "#print(concatenated_event_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Artifact Removal\n",
    "### Use Silence Periods Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single channel trace to store the x coordinates on key_press (for remove artifact function)\n",
    "coordinates_x_end = [] #holds the end timestamp of the short artifact (< 10ms)\n",
    "coordinates_x_long = [] #holds the start and end timestamp of a long artifact (> 10ms). Usually ~1 sec\n",
    "start_time = 5198 * fs\n",
    "end_time = 5199 * fs\n",
    "fig, ax = plt.subplots()\n",
    "trace = recording_f_cmr_prb.get_traces(start_frame=start_time, end_frame=end_time, channel_ids=[8], return_scaled=True)\n",
    "time_axis = np.arange(start_time, end_time) / fs\n",
    "ax.plot(time_axis, trace)\n",
    "\n",
    "# Initialize a variable to keep track of the column position (0 or 1)\n",
    "click_count = 0\n",
    "# Function to capture click events\n",
    "def onkey(event):\n",
    "    global click_count\n",
    "    if event.key == 'z': # Only respond to the \"z\" key\n",
    "        if event.xdata is not None:\n",
    "            coordinates_x_end.append((event.xdata))# Store the key_press coordinates in the list\n",
    "            print(f\"Key 'z' pressed at: x={event.xdata}\")# Display the coordinates\n",
    "                \n",
    "    elif event.key == 'w': # Only respond to the \"a\" key\n",
    "        if event.xdata is not None:\n",
    "            if click_count % 2 == 0:\n",
    "                # Start a new row for each pair of clicks\n",
    "                coordinates_x_long.append([event.xdata, None])  # Initialize the second column as None\n",
    "            else:\n",
    "                # Update the second column for the latest row\n",
    "                coordinates_x_long[-1][1] = event.xdata\n",
    "            print(f\"Key 'w' pressed at: x={event.xdata}\")\n",
    "            click_count += 1\n",
    "\n",
    "# Connect the button press event to the figure\n",
    "cid = fig.canvas.mpl_connect('key_press_event', onkey)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays with times in seconds for artifact removal. Remove 10 ms for TTL artifacts. \n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "sec = 0.01 # 10ms\n",
    "arr = np.array([coordinates_x_end][0], dtype=np.float32)\n",
    "new_col = arr - sec\n",
    "coordinates_end = np.column_stack((new_col, arr))\n",
    "coordinates_long = np.array([coordinates_x_long][0], dtype=np.float32)\n",
    "artifacts_coordinates_x = np.row_stack((coordinates_end, coordinates_long))\n",
    "# Create new csv file with values (6 decimal points), and append new values to existing csv file\n",
    "with open(data_folder / filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in artifacts_coordinates_x:\n",
    "                writer.writerow([f\"{value:.6f}\" for value in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples of triggers in frames from artifacts_coordinates_x csv file\n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "triggers_in_frames = []\n",
    "with open(data_folder / filename, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "         triggers_in_frames.append([((float(row[0])*fs)), ((float(row[1])*fs))])\n",
    "triggers_in_frames.sort()\n",
    "print(triggers_in_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean recording with silence_periods. List periods is one list per segment of tuples (start_frame, end_frame).\n",
    "recording_f_cmr_prb_clean = si.silence_periods(recording_f_cmr_prb, list_periods=triggers_in_frames, seed=0, mode='zeros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_layers1 = dict(common=recording_f_cmr, \n",
    "                        clean=recording_f_cmr_prb_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = si.plot_traces(recording_layers1, time_range=[6000, 6670], channel_ids=[8],\n",
    "                return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Motion correction, followed by Whiten\n",
    "https://spikeinterface.readthedocs.io/en/stable/how_to/handle_drift.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preset_params_ks = si.get_motion_parameters_preset(\"kilosort_like\")\n",
    "preset = \"kilosort_like\"\n",
    "preset_params_ks = ({'direction': 'y', 'rigid': False, 'win_shape': 'rect', 'win_step_um': 100.0, 'win_scale_um': 200.0, 'win_margin_um': None, \n",
    "                     'bin_um': 10.0, 'hist_margin_um': 0, 'bin_s': 2.0, 'num_amp_bins': 20, 'num_shifts_global': 15, 'num_iterations': 10, 'num_shifts_block': 5, \n",
    "                     'smoothing_sigma': 0.5, 'kriging_sigma': 1, 'kriging_p': 2, 'kriging_d': 2, 'method': 'iterative_template'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing with\", preset)\n",
    "folder = data_folder / \"motion_folder_dataset\" / preset\n",
    "recording_corrected, motion, motion_info = si.correct_motion(recording_f_cmr_prb_clean, preset=preset, folder=folder, output_motion=True, \n",
    "                                                                 output_motion_info=True, estimate_motion_kwargs=preset_params_ks, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = data_folder / \"motion_folder_dataset\" / preset\n",
    "motion_info = si.load_motion_info(folder)\n",
    "# plot motion\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "si.plot_motion_info(motion_info, recording_f_cmr_prb_clean, figure=fig, depth_lim=(-50, 300), \n",
    "                    color_amplitude=True, amplitude_cmap=\"inferno\", scatter_decimate=10)\n",
    "fig.suptitle(f\"{preset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.sortingcomponents.motion import correct_motion_on_peaks\n",
    "\n",
    "folder = data_folder / \"motion_folder_dataset\" / preset\n",
    "motion_info = si.load_motion_info(folder)\n",
    "motion = motion_info[\"motion\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 6), sharey=True)\n",
    "ax = axs[0]\n",
    "si.plot_probe_map(recording_f_cmr_prb_clean, ax=ax)\n",
    "peaks = motion_info[\"peaks\"]\n",
    "sr = rec.get_sampling_frequency()\n",
    "time_lim0 = 0.0\n",
    "time_lim1 = 3000.0\n",
    "mask = (peaks[\"sample_index\"] > int(sr * time_lim0)) & (peaks[\"sample_index\"] < int(sr * time_lim1))\n",
    "sl = slice(None, None, 5)\n",
    "amps = np.abs(peaks[\"amplitude\"][mask][sl])\n",
    "amps /= np.quantile(amps, 0.95)\n",
    "c = plt.get_cmap(\"inferno\")(amps)\n",
    "\n",
    "color_kargs = dict(alpha=0.5, s=2, c=c)\n",
    "\n",
    "peak_locations = motion_info[\"peak_locations\"]\n",
    "ax.scatter(peak_locations[\"x\"][mask][sl], peak_locations[\"y\"][mask][sl], **color_kargs)\n",
    "ax.set_ylim(-75, 250)\n",
    "ax.set_xlim(-45, 80)\n",
    "\n",
    "peak_locations2 = correct_motion_on_peaks(peaks, peak_locations, motion,rec)\n",
    "ax = axs[1]\n",
    "si.plot_probe_map(recording_f_cmr_prb_clean, ax=ax)\n",
    "ax.scatter(peak_locations2[\"x\"][mask][sl], peak_locations2[\"y\"][mask][sl], **color_kargs)\n",
    "ax.set_ylim(-75, 320)\n",
    "ax.set_xlim(-45, 80)\n",
    "fig.suptitle(f\"{preset=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_f_cmr_prb_clean_corr_w = si.whiten(recording_corrected, int_scale=200)\n",
    "\n",
    "recording_layers = dict(common=recording_f_cmr, \n",
    "                        clean=recording_f_cmr_prb_clean,\n",
    "                        whiten=recording_f_cmr_prb_clean_corr_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = si.plot_traces(recording_layers, time_range=[350, 360], channel_ids=[8, 45],\n",
    "                return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Saving filtered, clean, motion corrected and whitened recording (with probe) to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_f_cmr_prb_clean_corr_w.save(format='binary', folder=data_folder / \"recording_preprocessed\", overwrite=True, **job_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Si_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
