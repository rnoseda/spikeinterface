{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.2 - Adapted by Rodrigo Noseda - November 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recording and Paths <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "\n",
    "recording_paths_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.startswith('RawEphysData') and filename.endswith('.bin'):\n",
    "        recording_paths_list.append(data_folder / filename)\n",
    "print('Recording Files List:')\n",
    "print(recording_paths_list)\n",
    "\n",
    "# parameters associated to the recording in bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "rec_dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 2 #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concatenate Recording, Filtering, CMR and Whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and append recording segments to Baserecording object\n",
    "recordings_list = []\n",
    "rec = si.read_binary(recording_paths_list, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=False)\n",
    "recordings_list.append(rec)#Appends all extracted rec to a list. Kilosort does not support segments. Use concatenation or sort segments separatedly.\n",
    "#recording = si.append_recordings(recordings_list)#Creates Object AppendSegmentRecording\n",
    "recording = si.concatenate_recordings(recordings_list)#Creates Object ConcatenateSegmentRecording\n",
    "#Filtering recording\n",
    "recording_f = si.bandpass_filter(recording, freq_min=400, freq_max=5000)\n",
    "recording_f_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "recording_f_cmr_w = si.whiten(recording_f_cmr, int_scale=200)\n",
    "#recording_f_cmr_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checktraces before saving preprocessed recording (f, cmr, w, clean) to binary\n",
    "recording_f_cmr_w_seg1 = si.select_segment_recording(recording_f_cmr_w, segment_indices=0)\n",
    "recording_f_cmr_w_seg2 = si.select_segment_recording(recording_f_cmr_w, segment_indices=1)\n",
    "\n",
    "recording_layers = dict(seg1=recording_f_cmr_w_seg1, seg2=recording_f_cmr_w_seg2)\n",
    "\n",
    "w = si.plot_traces(recording_layers, time_range=[300, 500], channel_ids=[8], return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = recording_f_cmr_w.get_times(segment_index=0)\n",
    "b = recording_f_cmr_w_seg1.get_times()\n",
    "c = recording_f_cmr_w_seg2.get_times()\n",
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Artifact Removal\n",
    "### Use Silence Periods Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single channel trace to store the x coordinates on key_press (for remove artifact function)\n",
    "coordinates_x_end = [] #holds the end timestamp of the short artifact (< 10ms)\n",
    "coordinates_x_long = [] #holds the start and end timestamp of a long artifact (> 10ms). Usually ~1 sec\n",
    "start_time = 568 * fs\n",
    "end_time = 571 * fs\n",
    "fig, ax = plt.subplots()\n",
    "trace = recording_f_cmr_w.get_traces(start_frame=start_time, end_frame=end_time, channel_ids=[42], return_scaled=True)\n",
    "time_axis = np.arange(start_time, end_time) / fs\n",
    "ax.plot(time_axis, trace)\n",
    "\n",
    "# Initialize a variable to keep track of the column position (0 or 1)\n",
    "click_count = 0\n",
    "# Function to capture click events\n",
    "def onkey(event):\n",
    "    global click_count\n",
    "    if event.key == 'z': # Only respond to the \"z\" key\n",
    "        if event.xdata is not None:\n",
    "            coordinates_x_end.append((event.xdata))# Store the key_press coordinates in the list\n",
    "            print(f\"Key 'z' pressed at: x={event.xdata}\")# Display the coordinates\n",
    "                \n",
    "    elif event.key == 'w': # Only respond to the \"a\" key\n",
    "        if event.xdata is not None:\n",
    "            if click_count % 2 == 0:\n",
    "                # Start a new row for each pair of clicks\n",
    "                coordinates_x_long.append([event.xdata, None])  # Initialize the second column as None\n",
    "            else:\n",
    "                # Update the second column for the latest row\n",
    "                coordinates_x_long[-1][1] = event.xdata\n",
    "            print(f\"Key 'w' pressed at: x={event.xdata}\")\n",
    "            click_count += 1\n",
    "\n",
    "# Connect the button press event to the figure\n",
    "cid = fig.canvas.mpl_connect('key_press_event', onkey)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays with times in seconds for artifact removal. Remove 10 ms for TTL artifacts. \n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "sec = 0.01 # 10ms\n",
    "arr = np.array([coordinates_x_end][0])\n",
    "new_col = arr - sec\n",
    "coordinates_end = np.column_stack((new_col, arr))\n",
    "coordinates_long = np.array([coordinates_x_long][0])\n",
    "artifacts_coordinates_x = np.row_stack((coordinates_end, coordinates_long))\n",
    "# Create new csv file with values (6 decimal points), and append new values to existing csv file\n",
    "with open(data_folder / filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in artifacts_coordinates_x:\n",
    "                writer.writerow([f\"{value:.6f}\" for value in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split artifact times per segment for artifact removal and sorting\n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "triggers_in_frames = []\n",
    "df = pd.read_csv(data_folder / filename, header=None)\n",
    "df = df.sort_values(by=0).reset_index(drop=True)\n",
    "#artifact_times = pd.DataFrame({'Start_artifact': df.iloc[:, 0], 'End_artifact': df.iloc[:, 1]})\n",
    "split_index1 = 461\n",
    "#split_index2 = 461 #use if more than 2 segments\n",
    "df2 = df.iloc[:split_index1]\n",
    "df3 = df.iloc[split_index1:]\n",
    "# Save each segment to separate files\n",
    "df2.to_csv(data_folder / \"artifacts_segment_1.csv\", index=False, header=None)\n",
    "df3.to_csv(data_folder / \"artifacts_segment_2.csv\", index=False, header=None)\n",
    "\n",
    "#Load the two CSV files created earlier\n",
    "#segment_1 = pd.read_csv(\"segment_1.csv\")\n",
    "#segment_2 = pd.read_csv(\"segment_2.csv\")\n",
    "\n",
    "# Create a list containing the two DataFrames\n",
    "#segments_list = [segment_1, segment_2]\n",
    "\n",
    "# Print or return the list of segments\n",
    "#print(segments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples of triggers in frames from artifacts_coordinates_x csv file\n",
    "filename1 = 'artifacts_segment_1.csv'\n",
    "filename2 = 'artifacts_segment_2.csv'\n",
    "triggers_in_frames_seg1 = []\n",
    "triggers_in_frames_seg2 = []\n",
    "with open(data_folder / filename1, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "         triggers_in_frames_seg1.append([((float(row[0])*fs)), ((float(row[1])*fs))])\n",
    "with open(data_folder / filename2, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "         triggers_in_frames_seg2.append([((float(row[0])*fs)), ((float(row[1])*fs))])\n",
    "triggers_in_frames = [triggers_in_frames_seg1, triggers_in_frames_seg2]\n",
    "print(triggers_in_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuples of triggers in frames from artifacts_coordinates_x csv file\n",
    "filename = 'artifacts_coordinates_x.csv'\n",
    "triggers_in_frames = []\n",
    "with open(data_folder / filename, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "         triggers_in_frames.append([((float(row[0])*fs)), ((float(row[1])*fs))])\n",
    "triggers_in_frames.sort()\n",
    "print(triggers_in_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean recording with silence_periods. List periods is one list per segment of tuples (start_frame, end_frame).\n",
    "recording_f_cmr_w_clean = si.silence_periods(recording_f_cmr_w, list_periods=triggers_in_frames, seed=0, mode='zeros')\n",
    "\n",
    "#recording_layers = dict(filtered=recording_f_cmr, whitened=recording_f_cmr_w, w_cleaned=recording_f_cmr_w_clean,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checktraces before saving preprocessed recording (f, cmr, w, clean) to binary\n",
    "w0 = si.plot_traces(recording_f_cmr_w, time_range=[0, 1], channel_ids=[8], segment_index=0,\n",
    "                   return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Saving filtered, clean, motion corrected and whitened recording (with probe) to binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_f_cmr_w_clean.save(format='binary', folder=data_folder / \"recording_preprocessed_200\", overwrite=True, **job_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Si_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
