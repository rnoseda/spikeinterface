{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.2 - Adapted by Rodrigo Noseda - October 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading saved recording and probe information <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "\n",
    "# parameters associated to the recording in bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "rec_dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 2 #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "# Loading filtered and cleaned recording from binary saved.\n",
    "recording_filename = data_folder / 'recording_clean' / 'traces_cached_seg0.raw'\n",
    "recording_loaded = si.read_binary(recording_filename, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=True)\n",
    "#recording_loaded = si.load_extractor(file_or_folder, base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probe from library and set channel mapping\n",
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "print(f\"ProbeInterface version: {pi.__version__}\")\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-158-H10' #probe_name = 'ASSY-158-F' #probe_name = 'ASSY-158-H6'\n",
    "probeH10 = pi.get_probe(manufacturer, probe_name)#library: comes with contact_ids and shank_ids info.\n",
    "\n",
    "#Mapping Intan (device) channels\n",
    "device_channel_indices = [24,23,25,22,26,21,27,20,28,19,29,18,30,17,31,16,0,15,1,14,2,13,3,12,4,11,5,10,6,9,7,8,\n",
    "    56,55,57,54,58,53,59,52,60,51,61,50,62,49,63,48,32,47,33,46,34,45,35,44,36,43,37,42,38,41,39,40] #Modify accordingly.\n",
    "#   88,87,89,86,90,85,91,84,92,83,93,82,94,81,95,80,64,79,65,78,66,77,67,76,68,75,69,74,70,73,71,72,\n",
    "#   120,119,121,118,122,117,123,116,124,115,125,114,126,113,127,112,96,111,97,110,98,109,99,108,100,107,101,106,102,105,103,104]\n",
    "#Setting Intan channels to probe(RHD-2132/2164)\n",
    "probeH10.set_device_channel_indices(device_channel_indices)\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "plot_probe(probeH10, ax=ax, with_contact_id=True, with_device_index=True,)\n",
    "ax.set_xlim(-20, 200)\n",
    "ax.set_ylim(-60, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set and group by shank probe before sorting\n",
    "recording_clean_prb = recording_loaded.set_probe(probeH10, group_mode=\"by_shank\")\n",
    "recordings_by_group = recording_clean_prb.split_by(\"group\")\n",
    "recording_to_process = recordings_by_group[0]\n",
    "recording_to_process = recording_to_process.time_slice(start_time=0, end_time=600)\n",
    "\n",
    "#Load times from timestamps csv files and calculate start time in seconds.\n",
    "tms_files = sorted(glob.glob(os.path.join(data_folder, \"Timestamps*.csv\")))\n",
    "concatenated_start_times = pd.DataFrame()\n",
    "for tms_file in tms_files:\n",
    "    df = pd.read_csv(tms_file, header=None, nrows=1, names=['Start_Times'])#(usecols=[0], nrows=1)\n",
    "    df['Start_Times'] = df['Start_Times'].str.slice(0, 15)#first = df.head(1) #last = df.tail(1)\n",
    "    concatenated_start_times = pd.concat([concatenated_start_times, df], ignore_index=True)\n",
    "concatenated_start_times['Start_Times'] = pd.to_datetime(concatenated_start_times['Start_Times'])\n",
    "time_diff = concatenated_start_times['Start_Times'] - concatenated_start_times['Start_Times'].iloc[0]\n",
    "seconds_start = time_diff.dt.total_seconds()\n",
    "#print(seconds_start)\n",
    "\n",
    "#Get and set time vector, and confirm recording_to_process features\n",
    "for i in range(recording_to_process.get_num_segments()):\n",
    "    s = recording_to_process.get_num_samples(segment_index=i)\n",
    "    d = recording_to_process.get_duration(segment_index=i)\n",
    "    t = recording_to_process.get_times(segment_index=i)\n",
    "    p = recording_to_process.has_probe()\n",
    "    tms_temp = t + seconds_start[i]\n",
    "    tms = recording_to_process.set_times(tms_temp, segment_index=i, with_warning=True)\n",
    "    tv = recording_to_process.has_time_vector(segment_index=i)\n",
    "    print(f\"Segment {i}: Duration: {d} sec - Samples: {s} - Has time vector?: {tv} - Has Probe?: {p} - Time Vector: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spike sorting <a class=\"anchor\" id=\"spike-sorting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "default_KS4_params = si.get_default_sorter_params('kilosort4')\n",
    "# Parameters can be changed by single arguments: \n",
    "default_KS4_params['batch_size'] = 150000 #5 sec\n",
    "default_KS4_params['nblocks'] = 0 \n",
    "default_KS4_params['Th_universal'] = 8\n",
    "default_KS4_params['Th_learned'] = 7\n",
    "default_KS4_params['nearest_chans'] = 10 \n",
    "default_KS4_params['nearest_templates'] = 32\n",
    "#default_KS4_params['artifact_threshold'] = 10\n",
    "default_KS4_params['dmin'] = 30\n",
    "default_KS4_params['dminx'] = 30\n",
    "default_KS4_params['min_template_size'] = 10\n",
    "default_KS4_params['scale'] = 2\n",
    "#default_KS4_params['do_CAR'] = True\n",
    "default_KS4_params['skip_kilosort_preprocessing'] = False\n",
    "default_KS4_params['do_correction'] = True\n",
    "#default_KS4_params['duplicate_spike_ms'] = 0.5\n",
    "sorter_params = default_KS4_params\n",
    "pprint(default_KS4_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run spike sorting on recording using docker container\n",
    "sorting_KS4 = si.run_sorter('kilosort4', recording_to_process, folder=data_folder / 'sorting_KS4_s0',\n",
    "                            docker_image=True, verbose=True, **sorter_params)#, **job_kwargs)\n",
    "print(sorting_KS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_rs = si.plot_rasters(sorting_KS4, time_range=(0, 590), backend='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Postprocessing: SortingAnalyzer <a class=\"anchor\" id=\"sortinganalyzer\"></a>\n",
    "\n",
    "The core module uses `SortingAnalyzer` for postprocessing computation from paired recording-sorting objects. It retrieves waveforms, templates, spike amplitudes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = si.estimate_sparsity(sorting_KS4,recording_to_process, num_spikes_for_sparsity=100, method=\"radius\",\n",
    "                               radius_um=100, peak_sign=\"neg\", amplitude_mode=\"extremum\")\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = si.create_sorting_analyzer(sorting_KS4, recording_to_process, folder=data_folder / \"sorting_analyzer_KS4\", \n",
    "                              format=\"binary_folder\", sparsity=sparsity, overwrite=True, **job_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Extensions: PCA, waveforms, templates, spike amplitude, correlograms, etc.\n",
    "\n",
    "Let's move on to explore the postprocessing capabilities of the `postprocessing` module. Similarly to the `SortingAnalizer` object, the method 'compute` retrieve info on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SortingAnalizer computations: each call will recompute and overwrite previous computations\n",
    "rand = sa.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)#subsample to create a template\n",
    "wf = sa.compute(\"waveforms\", ms_before=1.0, ms_after=2.0, **job_kwargs)\n",
    "templ =sa.compute(\"templates\", ms_before=1.0, ms_after=2.0, operators=[\"average\", \"std\"])#from raw waveforms or random_spikes\n",
    "spk_amp = sa.compute(\"spike_amplitudes\", peak_sign=\"neg\")#based on templates\n",
    "noise = sa.compute(\"noise_levels\")\n",
    "amp_scal = sa.compute(\"amplitude_scalings\")#per channel\n",
    "pca = sa.compute(\"principal_components\", n_components=6, whiten=False, mode=\"by_channel_local\", dtype='float32')\n",
    "corr = sa.compute(\"correlograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "isi = sa.compute(\"isi_histograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "spk_loc = sa.compute(\"spike_locations\", ms_before=0.5, ms_after=0.5, method=\"center_of_mass\")#need for drift metrics (drift_ptp, drift_std, drift_mad)\n",
    "templ_sim = sa.compute(\"template_similarity\", method=\"cosine\", )#need for spikeinterface_gui\n",
    "u_loc = sa.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "templ_metric = sa.compute(\"template_metrics\", include_multi_channel_metrics=True)\n",
    "qm = sa.compute(\"quality_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Quality Metrics <a class=\"anchor\" id=\"qualitymetrics\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_template_similarity(sa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.get_default_qm_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitud cutoff (calculate the approximate fraction of missing spikes) - Need \"spike_amplitudes\"\n",
    "fraction_missing = si.compute_amplitude_cutoffs(sa, peak_sign=\"neg\")\n",
    "\n",
    "#Amplitud CV (coefficient of variation) - Need \"spike_amplitudes\" or \"amplitude_scalings\" pre-computed.\n",
    "amplitude_cv_median, amplitude_cv_range = si.compute_amplitude_cv_metrics(sa)\n",
    "#dicts: unit ids as keys, and amplitude_cv metrics as values.\n",
    "\n",
    "#Drift metrics - Need \"spike_locations\"\n",
    "drift_ptps, drift_stds, drift_mads = si.compute_drift_metrics(sa)\n",
    "#dicts: unit ids as keys, and drifts metrics as values.\n",
    "\n",
    "#Firing Range (outside of physiological range, might indicate noise contamination)\n",
    "firing_range = si.compute_firing_ranges(sa)\n",
    "#dict: unit IDs as keys, firing_range as values (in Hz).\n",
    "\n",
    "#***Firing Rate (average number of spikes/sec within the recording)\n",
    "firing_rate = si.compute_firing_rates(sa)\n",
    "#dict or floats: unit IDs as keys, firing rates across segments as values (in Hz).\n",
    "\n",
    "#Inter-spike-interval (ISI) Violations (rate of refractory period violations)\n",
    "isi_violations_ratio, isi_violations_count = si.compute_isi_violations(sa, isi_threshold_ms=1.5) \n",
    "#dicts: unit ids as keys, and isi ratio viol and number of viol as values.\n",
    "\n",
    "#****Refractory period Violations\n",
    "rp_violation = si.compute_refrac_period_violations(sa, refractory_period_ms=1.0, censored_period_ms=0.0)\n",
    "\n",
    "#Presence Ratio (proportion of discrete time bins in which at least one spike occurred)\n",
    "presence_ratio = si.compute_presence_ratios(sa)\n",
    "#dict: unit IDs as keys, presence ratio (between 0 and 1) as values.\n",
    "#Close or > 0.9 = complete units.\n",
    "#Close to 0 = incompleteness (type II error) or highly selective firing pattern.\n",
    "\n",
    "#****Standard Deviation (SD) ratio\n",
    "sd_ratio = si.compute_sd_ratio(sa, censored_period_ms=4.0)\n",
    "#Close to 1 = unit from single neuron.\n",
    "\n",
    "#****Signal-to-noise ratio (SNR)\n",
    "SNRs = si.compute_snrs(sa)\n",
    "#dict: unit IDs as keys and their SNRs as values.\n",
    "#High SNR = likely to correspond to a neuron. Low SNR = unit contaminated.\n",
    "\n",
    "#Synchrony Metrics (characterize synchronous events within the same spike train and across different spike trains)\n",
    "synchrony = si.compute_synchrony_metrics(sa, synchrony_sizes=(2, 4, 8))\n",
    "#tuple of dicts with the synchrony metrics for each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pcs = si.compute_principal_components(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Distance (distance from a cluster to the nearest other cluster)\n",
    "iso_distance = si.pca_metrics.mahalanobis_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns floats: iso_distance, l_ratio.\n",
    "\n",
    "#Nearest Neighbor Metrics (evaluate unit quality)\n",
    "si.pca_metrics.nearest_neighbors_metrics(all_pcs, all_labels, this_unit_id, max_spikes, n_neighbors)\n",
    "#Calculate unit contamination based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_isolation(sa)\n",
    "#Calculate unit isolation based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_noise_overlap(sa)\n",
    "#Calculate unit noise overlap based on NearestNeighbors search in PCA space.\n",
    "\n",
    "#D-prime (estimate the classification accuracy between two units)\n",
    "d_prime = si.lda_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns a float (larger in well separated clusters)\n",
    "\n",
    "#Silhouette score (ratio between the cohesiveness of a cluster and its separation from other clusters)\n",
    "simple_sil_score = si.simplified_silhouette_score(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#Close to 1 = good clustering. Close to -1 = poorly isolated cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the metrics on the good units, we can create rules to only keep units that are of sufficient quality. For example:\n",
    "- A `firing_rate` greater than 1.0 Hz\n",
    "- A `snr` greater than 1.1\n",
    "- A `rp_contamination` below 20%\n",
    "- A `sd_ratio` below 1.5\n",
    "\n",
    "A straightforward way to filter a pandas dataframe is via the `query`.\n",
    "We first define our query (make sure the names match the column names of the dataframe):\n",
    "and then we can use the query to select units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I know quality of units, add them below.\n",
    "good_unit_ids = np.array([3, 13, 19, 34, 39, 40, 41], dtype=np.int32)\n",
    "ok_unit_ids = np.array([11, 18, 22, 51], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule = \"firing_rate > 1.0 & snr > 1.1  & rp_contamination < 0.2 & sd_ratio < 1.5\"\n",
    "good_metrics = qm.query(rule)\n",
    "curated_unit_ids = list(good_metrics.index)\n",
    "print(curated_unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_sorting = sorting_KS4.select_units(curated_unit_ids)\n",
    "curated_analyzer = sa.select_units(curated_unit_ids)\n",
    "curated_sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = si.get_potential_auto_merge(curated_analyzer)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    si.plot_crosscorrelograms(sa, unit_ids=pair, min_similarity_for_correlograms=None, backend=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 9. Export to Phy for manual curation <a class=\"anchor\" id=\"exporters\"></a>\n",
    "#### [Phy](https://github.com/cortex-lab/phy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_to_phy(sa, output_folder=data_folder / 'phy_KS4_RN', compute_pc_features=True,\n",
    "                   copy_binary=True, dtype='float32', compute_amplitudes=True,\n",
    "                   sparsity=sparsity, add_quality_metrics=True, add_template_metrics=True, \n",
    "                   template_mode='median', verbose=True,**job_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Si_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
