{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.2 - Adapted by Rodrigo Noseda - October 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading recording and probe information <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "#Pasted directly from explorer \"C:\\Users\\rodri\\Documents\\Bonsai-RN\\Bonsai_DataRN\\2023_3_21\\\"\n",
    "\n",
    "recording_paths_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.startswith('RawEphysData') and filename.endswith('.bin'):\n",
    "        recording_paths_list.append(data_folder / filename)\n",
    "print('Recording Files List:')\n",
    "print(recording_paths_list)\n",
    "\n",
    "# parameters associated to the recording in bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "rec_dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "n_jobs = -1#-1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract and append recording segments to Baserecording object\n",
    "recordings_list = []\n",
    "rec = si.read_binary(recording_paths_list, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=False)\n",
    "recordings_list.append(rec)#Appends all extracted rec to a list. Kilosort does not support segments. Use concatenation.\n",
    "recording = si.concatenate_recordings(recordings_list)#Creates Object ConcatenateSegmentRecording"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n",
    "All preprocessing modules return new `RecordingExtractor` objects that apply the underlying preprocessing function. This allows users to access the preprocessed data in the same way as the raw data. We will focus only on the first shank (group `0`) for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recording_f = si.bandpass_filter(recording, freq_min=300, freq_max=6000)\n",
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "recording_layers = [recording, recording_f, recording_cmr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_traces(recording_layers, time_range=[390, 420], channel_ids=[8, 39, 45, 63],\n",
    "                    return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels = ['art1', 'art2', 'art3', 'art4']\n",
    "artifacts = {'art1': [390.0, 420.7], 'art2': [5 , 45], 'art3': [569.2, 599.6], 'art4': [90, 91]}\n",
    "list_periods = [(17073622, 17976622)]\n",
    "#One list per segment of tuples (start_frame, end_frame) to silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "recording_clean = si.silence_periods(recording_f, list_periods=list_periods, seed=0, mode='zeros')\n",
    "#recording_clean = si.remove_artifacts(recording_f, list_triggers=triggers_in_frames, ms_before=14, ms_after=1, mode='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_clean.save(format='binary', folder=data_folder / \"recording_f_clean\", **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probe from library and set channel mapping\n",
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "print(f\"ProbeInterface version: {pi.__version__}\")\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-158-H10' #probe_name = 'ASSY-158-F' #probe_name = 'ASSY-158-H6'\n",
    "probeH10 = pi.get_probe(manufacturer, probe_name)#library: comes with contact_ids and shank_ids info.\n",
    "\n",
    "#Mapping Intan (device) channels\n",
    "device_channel_indices = [24,23,25,22,26,21,27,20,28,19,29,18,30,17,31,16,0,15,1,14,2,13,3,12,4,11,5,10,6,9,7,8,\n",
    "    56,55,57,54,58,53,59,52,60,51,61,50,62,49,63,48,32,47,33,46,34,45,35,44,36,43,37,42,38,41,39,40] #Modify accordingly.\n",
    "#   88,87,89,86,90,85,91,84,92,83,93,82,94,81,95,80,64,79,65,78,66,77,67,76,68,75,69,74,70,73,71,72,\n",
    "#   120,119,121,118,122,117,123,116,124,115,125,114,126,113,127,112,96,111,97,110,98,109,99,108,100,107,101,106,102,105,103,104]\n",
    "#Setting Intan channels to probe(RHD-2132/2164)\n",
    "probeH10.set_device_channel_indices(device_channel_indices) #print(probeH10.device_channel_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'traces_cached_seg0.raw'\n",
    "#recording_f_clean = data_folder / 'recording_f_clean2' / filename\n",
    "#recording_loaded = si.read_binary(recording_f_clean, num_chan=num_channels,sampling_frequency=fs,\n",
    "#                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "#                           time_axis=time_axis, is_filtered=True)\n",
    "\n",
    "recording_clean_prb = recording_clean.set_probe(probeH10, group_mode=\"by_shank\")\n",
    "recordings_by_group = recording_clean_prb.split_by(\"group\")\n",
    "recording_to_process = recordings_by_group[0]\n",
    "recording_to_process_short = recording_to_process.time_slice(start_time=0, end_time=600)\n",
    "print(recording_to_process_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_traces(recording_to_process_short, time_range=[0, 600], channel_ids=[8, 45, 63],\n",
    "                    return_scaled=True, show_channel_ids=True, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spike sorting <a class=\"anchor\" id=\"spike-sorting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "default_KS4_params = si.get_default_sorter_params('kilosort4')\n",
    "# Parameters can be changed by single arguments: \n",
    "default_KS4_params['batch_size'] = 60000 #2 sec\n",
    "default_KS4_params['nblocks'] = 0 \n",
    "#default_KS4_params['Th_universal'] = 8\n",
    "#default_KS4_params['Th_learned'] = 6\n",
    "#default_KS4_params['nearest_chans'] = 8 \n",
    "default_KS4_params['nearest_templates'] = 32\n",
    "#default_KS4_params['artifact_threshold'] = 20\n",
    "#default_KS4_params['dmin'] = 30\n",
    "#default_KS4_params['dminx'] = 30\n",
    "#default_KS4_params['min_template_size'] = 15\n",
    "#default_KS4_params['scale'] = 5\n",
    "#default_KS4_params['duplicate_spike_ms'] = 0.25\n",
    "default_KS4_params['skip_kilosort_preprocessing'] = True\n",
    "default_KS4_params['do_correction'] = False\n",
    "pprint(default_KS4_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run spike sorting on recording using docker container\n",
    "sorting_KS4_s0 = si.run_sorter('kilosort4', recording_to_process_short, folder=data_folder / 'sorting_KS4_shank0d',\n",
    "                            docker_image=True, verbose=True)#, **sorter_params, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorting_KS4_s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_rs = si.plot_rasters(sorting_KS4_s0, time_range=(0, 600), backend='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Postprocessing: SortingAnalyzer <a class=\"anchor\" id=\"sortinganalyzer\"></a>\n",
    "\n",
    "The core module uses `SortingAnalyzer` for postprocessing computation from paired recording-sorting objects. It retrieves waveforms, templates, spike amplitudes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparsity = si.estimate_sparsity?\n",
    "sparsity = si.estimate_sparsity(sorting_KS4_s0,recording_f_clean, num_spikes_for_sparsity=200, method=\"radius\",\n",
    "                                radius_um=100, peak_sign=\"neg\", amplitude_mode=\"extremum\")\n",
    "print(sparsity)\n",
    "#for unit_id in sparsity.unit_ids[::30]:\n",
    "#    print(unit_id, list(sparsity.unit_id_to_channel_ids[unit_id]))\n",
    "#most of the plotting, computation and export functions are using 'sparsity' in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si.create_sorting_analyzer?\n",
    "sa = si.create_sorting_analyzer(sorting_KS4_s0, recording_clean5, folder=data_folder / \"sorting_analyzer_KS4_s0\", \n",
    "                              format=\"binary_folder\", sparsity=sparsity, overwrite=True, **job_kwargs)\n",
    "#Saving Analyzer in specific format and loading it from saved\n",
    "#sa.save_as(format=\"zarr\",folder=data_folder / \"sorting_analyzer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Extensions: PCA, waveforms, templates, spike amplitude, correlograms, etc.\n",
    "\n",
    "Let's move on to explore the postprocessing capabilities of the `postprocessing` module. Similarly to the `SortingAnalizer` object, the method 'compute` retrieve info on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_computable_extensions = sa.get_computable_extensions()\n",
    "print(all_computable_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SortingAnalizer computations: each call will recompute and overwrite previous computations\n",
    "rand = sa.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)#subsample to create a template\n",
    "wf = sa.compute(\"waveforms\", ms_before=1, ms_after=2, **job_kwargs)\n",
    "templ =sa.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])#from raw waveforms or random_spikes\n",
    "spk_amp = sa.compute(\"spike_amplitudes\", peak_sign=\"neg\")#based on templates\n",
    "noise = sa.compute(\"noise_levels\")\n",
    "amp_scal = sa.compute(\"amplitude_scalings\")#per channel\n",
    "pca = sa.compute(\"principal_components\", n_components=3, mode=\"by_channel_local\")\n",
    "corr = sa.compute(\"correlograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "isi = sa.compute(\"isi_histograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "spk_loc = sa.compute(\"spike_locations\", method=\"center_of_mass\")#need for drift metrics (drift_ptp, drift_std, drift_mad)\n",
    "templ_sim = sa.compute(\"template_similarity\")#need for spikeinterface_gui\n",
    "u_loc = sa.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "templ_metric = sa.compute(\"template_metrics\")\n",
    "qm = sa.compute(\"quality_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity2 = si.compute_sparsity(sa,recording_f_clean)\n",
    "print(sparsity2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions are generally saved in two ways: \n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"memory\")\n",
    "\n",
    "sorting_analyzer.save_as(folder=\"my_sorting_analyzer\")\n",
    "sorting_analyzer.compute(\"random_spikes\", save=True)\n",
    "\n",
    "Here the random_spikes extension is not saved. The sorting_analyzer is still saved in memory. The save_as method only made a snapshot of the sorting analyzer which is saved in a folder. This is useful when trying out different parameters and initially setting up your pipeline. If we wanted to save the extension we should have started with a non-memory sorting analyzer:\n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"binary_folder\", folder=\"my_sorting_analyzer\")\n",
    "sorting_analyzer.compute(\"random_spikes\", save=True)\n",
    "\n",
    "NOTE: We recommend choosing a workflow and sticking with it. Either keep everything on disk or keep everything in memory until youâ€™d like to save. A mixture can lead to unexpected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Quality Metrics <a class=\"anchor\" id=\"qualitymetrics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.get_default_qm_params()\n",
    "si.get_quality_metric_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitud cutoff (calculate the approximate fraction of missing spikes)\n",
    "#Need \"spike_amplitudes\"\n",
    "fraction_missing = si.compute_amplitude_cutoffs(sa, peak_sign=\"neg\")\n",
    "\n",
    "#Amplitud CV (coefficient of variation)\n",
    "#Need \"spike_amplitudes\" or \"amplitude_scalings\" pre-computed.\n",
    "amplitude_cv_median, amplitude_cv_range = si.compute_amplitude_cv_metrics(sa)\n",
    "#dicts: unit ids as keys, and amplitude_cv metrics as values.\n",
    "\n",
    "#Drift metrics\n",
    "#Need \"spike_locations\"\n",
    "drift_ptps, drift_stds, drift_mads = si.compute_drift_metrics(sa)\n",
    "#dicts: unit ids as keys, and drifts metrics as values.\n",
    "\n",
    "#Firing Range (outside of physiological range, might indicate noise contamination)\n",
    "firing_range = si.compute_firing_ranges(sa)\n",
    "#dict: unit IDs as keys, firing_range as values (in Hz).\n",
    "\n",
    "#Firing Rate (average number of spikes/sec within the recording)\n",
    "firing_rate = si.compute_firing_rates(sa)\n",
    "#dict or floats: unit IDs as keys, firing rates across segments as values (in Hz).\n",
    "\n",
    "#Inter-spike-interval (ISI) Violations (rate of refractory period violations)\n",
    "isi_violations_ratio, isi_violations_count = si.compute_isi_violations(sa, isi_threshold_ms=1.5) \n",
    "#dicts: unit ids as keys, and isi ratio viol and number of viol as values.\n",
    "\n",
    "#Presence Ratio (proportion of discrete time bins in which at least one spike occurred)\n",
    "presence_ratio = si.compute_presence_ratios(sa)\n",
    "#dict: unit IDs as keys, presence ratio (between 0 and 1) as values.\n",
    "#Close or > 0.9 = complete units.\n",
    "#Close to 0 = incompleteness (type II error) or highly selective firing pattern.\n",
    "\n",
    "#Standard Deviation (SD) ratio\n",
    "sd_ratio = si.compute_sd_ratio(sa, censored_period_ms=4.0)\n",
    "#Close to 1 = unit from single neuron.\n",
    "\n",
    "#Signal-to-noise ratio (SNR)\n",
    "SNRs = si.compute_snrs(sa)\n",
    "#dict: unit IDs as keys and their SNRs as values.\n",
    "#High SNR = likely to correspond to a neuron. Low SNR = unit contaminated.\n",
    "\n",
    "#Synchrony Metrics (characterize synchronous events within the same spike train and across different spike trains)\n",
    "synchrony = si.compute_synchrony_metrics(sa, synchrony_sizes=(2, 4, 8))\n",
    "#tuple of dicts with the synchrony metrics for each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.get_quality_pca_metric_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Distance (distance from a cluster to the nearest other cluster)\n",
    "iso_distance = si.pca_metrics.mahalanobis_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns floats: iso_distance, l_ratio.\n",
    "\n",
    "#Nearest Neighbor Metrics (evaluate unit quality)\n",
    "si.pca_metrics.nearest_neighbors_metrics(all_pcs, all_labels, this_unit_id, max_spikes, n_neighbors)\n",
    "#Calculate unit contamination based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_isolation(sa)\n",
    "#Calculate unit isolation based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_noise_overlap(sa)\n",
    "#Calculate unit noise overlap based on NearestNeighbors search in PCA space.\n",
    "\n",
    "#D-prime (estimate the classification accuracy between two units)\n",
    "d_prime = si.lda_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns a float (larger in well separated clusters)\n",
    "\n",
    "#Silhouette score (ratio between the cohesiveness of a cluster and its separation from other clusters)\n",
    "simple_sil_score = si.simplified_silhouette_score(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#Close to 1 = good clustering. Close to -1 = poorly isolated cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward way to filter a pandas dataframe is via the `query`.\n",
    "We first define our query (make sure the names match the column names of the dataframe):\n",
    "and then we can use the query to select units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Automatic curation based on parameters\n",
    "isi_viol_thresh = 0.5\n",
    "amp_cutoff_thresh = 0.1\n",
    "\n",
    "#our_query = f\"amplitude_cutoff < {amp_cutoff_thresh} & isi_violations_ratio < {isi_viol_thresh}\"\n",
    "our_query = f\"isi_violations_ratio < {isi_viol_thresh}\"\n",
    "print(our_query)\n",
    "\n",
    "keep_units = df.query(our_query)\n",
    "keep_unit_ids = keep_units.index.values\n",
    "keep_unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_auto_KS4 = sorting_KS4_s0.select_units(keep_unit_ids)\n",
    "print(f\"Number of units before curation: {len(sorting_KS4_s0.get_unit_ids())}\")\n",
    "print(f\"Number of units after curation: {len(sorting_auto_KS4.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_curated = sa.select_units(keep_unit_ids)\n",
    "\n",
    "#Saving Analyzer in specific format and loading it from saved\n",
    "sa_curated_saved = sa_curated.save_as(format=\"zarr\", folder=data_folder / \"sorting_analyzer_curated.zarr\")\n",
    "print(sa_curated_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Viewers <a class=\"anchor\" id=\"viewers\"></a>\n",
    "### SpikeInterface GUI\n",
    "Can be run directly in a terminal with: \n",
    "sigui /path/to/analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%gui qt\n",
    "si.plot_sorting_summary(sorting_analyzer=sa, curation=True, backend='spikeinterface_gui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = si.plot_unit_templates(sa, backend=\"ipywidgets\")#templ\n",
    "waveforms = si.plot_unit_waveforms(sa, backend=\"ipywidgets\")#wf\n",
    "unit_locations = si.plot_unit_locations(sa, backend=\"ipywidgets\")#u_loc\n",
    "spk_locations = si.plot_spike_locations(sa, backend=\"ipywidgets\")#spk_loc\n",
    "spk_amplitude = si.plot_amplitudes(sa, backend=\"ipywidgets\")#spk_amp\n",
    "template_similarity = si.plot_template_similarity(sa)#, backend=\"ipywidgets\")#templ_sim\n",
    "autocorr = si.plot_autocorrelograms(sa, unit_ids=[0, 1 ,2])#, backend=\"ipywidgets\")#corr\n",
    "crosscorr = si.plot_crosscorrelograms(sa, unit_ids=[0, 1, 2])#, backend=\"ipywidgets\")#corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortingView\n",
    "Web-based, shareable (with link), sorter visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time initialization (alternate method)\n",
    "import kachery_cloud as kcl\n",
    "kcl.init()\n",
    "\n",
    "# Follow the instructions to associate the client with your GitHub user on the kachery-cloud network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = si.plot_sorting_summary(sa, curation=True, backend='sortingview')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 9. Exporters <a class=\"anchor\" id=\"exporters\"></a>\n",
    "#### Export to Phy for manual curation [Phy](https://github.com/cortex-lab/phy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_to_phy(sa, output_folder=data_folder / 'phy_KS4_RN_s0_clean5_full', compute_pc_features=True,\n",
    "                   copy_binary=True, dtype='float32', compute_amplitudes=True,\n",
    "                   sparsity=sparsity, add_quality_metrics=True, add_template_metrics=True, \n",
    "                   template_mode='median', verbose=True,**job_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After curating the results we can reload it using the `PhySortingExtractor` and exclude the units that we labeled as `noise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_phy_curated = si.read_phy(data_folder / 'phy_KS4_RN/', exclude_cluster_groups=['noise'])\n",
    "print(f\"Number of units before curation: {len(sorting_KS4.get_unit_ids())}\")\n",
    "print(f\"Number of units after curation: {len(sorting_phy_curated.get_unit_ids())}\")\n",
    "#Save the loaded curated phy into Spikeinterface object!!\n",
    "#si.export_report(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface_gui\n",
    "app = spikeinterface_gui.mkQApp() \n",
    "win = spikeinterface_gui.MainWindow(sa, curation=True)\n",
    "win.show()\n",
    "app.exec_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Si_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
