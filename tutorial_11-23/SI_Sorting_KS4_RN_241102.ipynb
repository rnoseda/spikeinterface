{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.2 - Adapted by Rodrigo Noseda - October 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading saved recording and probe information <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_28/\")\n",
    "\n",
    "# parameters associated to the recording in bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "rec_dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 2 #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "\n",
    "# Loading filtered and cleaned recording from binary saved.\n",
    "recording_filename = data_folder / 'recording_clean' / 'traces_cached_seg0.raw'\n",
    "recording_loaded = si.read_binary(recording_filename, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=rec_dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=True)\n",
    "#recording_loaded = si.load_extractor(file_or_folder, base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probe from library and set channel mapping\n",
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "print(f\"ProbeInterface version: {pi.__version__}\")\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-158-H10' #probe_name = 'ASSY-158-F' #probe_name = 'ASSY-158-H6'\n",
    "probeH10 = pi.get_probe(manufacturer, probe_name)#library: comes with contact_ids and shank_ids info.\n",
    "\n",
    "#Mapping Intan (device) channels\n",
    "device_channel_indices = [24,23,25,22,26,21,27,20,28,19,29,18,30,17,31,16,0,15,1,14,2,13,3,12,4,11,5,10,6,9,7,8,\n",
    "    56,55,57,54,58,53,59,52,60,51,61,50,62,49,63,48,32,47,33,46,34,45,35,44,36,43,37,42,38,41,39,40] #Modify accordingly.\n",
    "#   88,87,89,86,90,85,91,84,92,83,93,82,94,81,95,80,64,79,65,78,66,77,67,76,68,75,69,74,70,73,71,72,\n",
    "#   120,119,121,118,122,117,123,116,124,115,125,114,126,113,127,112,96,111,97,110,98,109,99,108,100,107,101,106,102,105,103,104]\n",
    "#Setting Intan channels to probe(RHD-2132/2164)\n",
    "probeH10.set_device_channel_indices(device_channel_indices)\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "plot_probe(probeH10, ax=ax, with_contact_id=True, with_device_index=True,)\n",
    "ax.set_xlim(-20, 200)\n",
    "ax.set_ylim(-60, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set and group by shank probe before sorting\n",
    "recording_clean_prb = recording_loaded.set_probe(probeH10, group_mode=\"by_probe\")\n",
    "recordings_by_group = recording_clean_prb.split_by(\"group\")\n",
    "recording_to_process = recordings_by_group[0]\n",
    "#recording_to_process = recording_to_process.time_slice(start_time=0, end_time=600)\n",
    "\n",
    "#Load times from timestamps csv files and calculate start time in seconds.\n",
    "tms_files = sorted(glob.glob(os.path.join(data_folder, \"Timestamps*.csv\")))\n",
    "concatenated_start_times = pd.DataFrame()\n",
    "for tms_file in tms_files:\n",
    "    df = pd.read_csv(tms_file, header=None, nrows=1, names=['Start_Times'])#(usecols=[0], nrows=1)\n",
    "    df['Start_Times'] = df['Start_Times'].str.slice(0, 15)#first = df.head(1) #last = df.tail(1)\n",
    "    concatenated_start_times = pd.concat([concatenated_start_times, df], ignore_index=True)\n",
    "concatenated_start_times['Start_Times'] = pd.to_datetime(concatenated_start_times['Start_Times'])\n",
    "time_diff = concatenated_start_times['Start_Times'] - concatenated_start_times['Start_Times'].iloc[0]\n",
    "seconds_start = time_diff.dt.total_seconds()\n",
    "#print(seconds_start)\n",
    "\n",
    "#Get and set time vector, and confirm recording_to_process features\n",
    "for i in range(recording_to_process.get_num_segments()):\n",
    "    s = recording_to_process.get_num_samples(segment_index=i)\n",
    "    d = recording_to_process.get_duration(segment_index=i)\n",
    "    t = recording_to_process.get_times(segment_index=i)\n",
    "    p = recording_to_process.has_probe()\n",
    "    tms_temp = t + seconds_start[i]\n",
    "    tms = recording_to_process.set_times(tms_temp, segment_index=i, with_warning=True)\n",
    "    tv = recording_to_process.has_time_vector(segment_index=i)\n",
    "    print(f\"Segment {i}: Duration: {d} sec - Samples: {s} - Has time vector?: {tv} - Has Probe?: {p} - Time Vector: {t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Spike sorting <a class=\"anchor\" id=\"spike-sorting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "default_KS4_params = si.get_default_sorter_params('kilosort4')\n",
    "# Parameters can be changed by single arguments: \n",
    "default_KS4_params['batch_size'] = 150000 #5 sec\n",
    "default_KS4_params['nblocks'] = 0 \n",
    "default_KS4_params['Th_universal'] = 8\n",
    "default_KS4_params['Th_learned'] = 7\n",
    "default_KS4_params['nearest_chans'] = 10 \n",
    "default_KS4_params['nearest_templates'] = 32\n",
    "default_KS4_params['artifact_threshold'] = 50\n",
    "default_KS4_params['dmin'] = 30\n",
    "default_KS4_params['dminx'] = 30\n",
    "default_KS4_params['min_template_size'] = 10\n",
    "default_KS4_params['scale'] = 3\n",
    "default_KS4_params['do_CAR'] = True\n",
    "default_KS4_params['skip_kilosort_preprocessing'] = False\n",
    "default_KS4_params['do_correction'] = True\n",
    "default_KS4_params['duplicate_spike_ms'] = 0.25\n",
    "sorter_params = default_KS4_params\n",
    "pprint(default_KS4_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run spike sorting on recording using docker container\n",
    "sorting_KS4 = si.run_sorter('kilosort4', recording_to_process, folder=data_folder / 'sorting_KS4',\n",
    "                            docker_image=True, verbose=True, **sorter_params)#, **job_kwargs)\n",
    "print(sorting_KS4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#sorting_KS4 = si.read_kilosort(data_folder / 'sorting_KS4' / 'sorter_output')\n",
    "w_rs = si.plot_rasters(sorting_KS4, time_range=(0, 590), backend='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Postprocessing: SortingAnalyzer <a class=\"anchor\" id=\"sortinganalyzer\"></a>\n",
    "\n",
    "The core module uses `SortingAnalyzer` for postprocessing computation from paired recording-sorting objects. It retrieves waveforms, templates, spike amplitudes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity = si.estimate_sparsity(sorting_KS4,recording_to_process, num_spikes_for_sparsity=100, method=\"radius\",\n",
    "                               radius_um=100, peak_sign=\"neg\", amplitude_mode=\"extremum\")\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sa = si.load_sorting_analyzer(data_folder / 'sorting_analyzer_KS4')\n",
    "sa = si.create_sorting_analyzer(sorting_KS4, recording_to_process, folder=data_folder / \"sorting_analyzer_KS4\", \n",
    "                              format=\"binary_folder\", sparsity=sparsity, overwrite=True, **job_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Extensions: PCA, waveforms, templates, spike amplitude, correlograms, etc.\n",
    "\n",
    "Let's move on to explore the postprocessing capabilities of the `postprocessing` module. Similarly to the `SortingAnalizer` object, the method 'compute` retrieve info on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SortingAnalizer computations: each call will recompute and overwrite previous computations\n",
    "rand = sa.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)#subsample to create a template\n",
    "wf = sa.compute(\"waveforms\", ms_before=1.0, ms_after=2.0, **job_kwargs)\n",
    "templ =sa.compute(\"templates\", ms_before=1.0, ms_after=2.0, operators=[\"average\", \"std\"])#from raw waveforms or random_spikes\n",
    "spk_amp = sa.compute(\"spike_amplitudes\", peak_sign=\"neg\")#based on templates\n",
    "noise = sa.compute(\"noise_levels\")\n",
    "#amp_scal = sa.compute(\"amplitude_scalings\")#per channel\n",
    "pca = sa.compute(\"principal_components\", n_components=3, mode=\"by_channel_local\")#, whiten=False, dtype='float32')\n",
    "corr = sa.compute(\"correlograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "isi = sa.compute(\"isi_histograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "spk_loc = sa.compute(\"spike_locations\", ms_before=0.5, ms_after=0.5, method=\"center_of_mass\")#method=\"monopolar_triangulation\"(slow but more acccurate) #need for drift metrics (drift_ptp, drift_std, drift_mad)\n",
    "templ_sim = sa.compute(\"template_similarity\", method=\"cosine_similarity\", )#need for spikeinterface_gui. Not well suited for high-density probes!\n",
    "u_loc = sa.compute(\"unit_locations\", method='center_of_mass')#method=\"monopolar_triangulation\")\n",
    "#templ_metric = sa.compute(\"template_metrics\", include_multi_channel_metrics=True) #good when analyzing spike shapes, depolarization slope, etc. \n",
    "qm = sa.compute(\"quality_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 9. Export to Phy for manual curation <a class=\"anchor\" id=\"exporters\"></a>\n",
    "#### [Phy](https://github.com/cortex-lab/phy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_to_phy(sa, output_folder=data_folder / 'phy_KS4_RN', compute_pc_features=True,\n",
    "                   copy_binary=True, dtype='float32', compute_amplitudes=True,\n",
    "                   sparsity=sparsity, add_quality_metrics=True, add_template_metrics=True, \n",
    "                   template_mode='median', verbose=True,**job_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Si_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
