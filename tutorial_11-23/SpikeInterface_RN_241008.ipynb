{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.1 - Adapted by Rodrigo Noseda - October 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpikeInterface Version: 0.101.1\n"
     ]
    }
   ],
   "source": [
    "import spikeinterface.full as si\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading recording and probe information <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording Files List:\n",
      "[WindowsPath('D:/Ephys_C2DRG/2023_9_19/RawEphysData_32Ch_ProbeF_Broken_0.bin'), WindowsPath('D:/Ephys_C2DRG/2023_9_19/RawEphysData_32Ch_ProbeF_Broken_1.bin')]\n"
     ]
    }
   ],
   "source": [
    "# file paths\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "#Pasted directly from explorer \"C:\\Users\\rodri\\Documents\\Bonsai-RN\\Bonsai_DataRN\\2023_3_21\\\"\n",
    "\n",
    "recording_paths_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.startswith('RawEphysData') and filename.endswith('.bin'):\n",
    "        recording_paths_list.append(data_folder / filename)\n",
    "\n",
    "print('Recording Files List:')\n",
    "print(recording_paths_list)\n",
    "n_files = len(recording_paths_list)  \n",
    "#Dinamically create 'recording_n' variables\n",
    "for i in range(n_files):\n",
    "    globals()[f'recording{i}'] = f\"{recording_paths_list[i]}\" \n",
    "\n",
    "# parameters associated to the bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "dtype = \"float32\"\n",
    "time_axis = 0     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordings_list = []\n",
    "rec = si.read_binary(recording_paths_list, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=False)\n",
    "recordings_list.append(rec)\n",
    "#Appending segments of recordings (Better because concatenate breaks timeline!)\n",
    "recording = si.append_recordings(recordings_list)\n",
    "print(recording)\n",
    "for i in range(recording.get_num_segments()):\n",
    "    s = recording.get_num_samples(segment_index=i)\n",
    "    print(f\"Segment {i}: num_samples {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseTime Files List:\n",
      "[WindowsPath('D:/Ephys_C2DRG/2023_9_19/TimestampsEphys_0.csv'), WindowsPath('D:/Ephys_C2DRG/2023_9_19/TimestampsEphys_1.csv')]\n",
      "basetime1\n"
     ]
    }
   ],
   "source": [
    "basetime_paths_list = []\n",
    "for filename_t in os.listdir(data_folder):\n",
    "    if filename_t.startswith('Timestamps') and filename_t.endswith('.csv'):\n",
    "        basetime_paths_list.append(data_folder / filename_t)\n",
    "\n",
    "print('BaseTime Files List:')\n",
    "print(basetime_paths_list)  \n",
    "\n",
    "n_files = len(basetime_paths_list)  \n",
    "#Dinamically create 'basetime_n' variables\n",
    "for i in range(n_files):\n",
    "    csv_file = Path(f\"basetime{i}\")\n",
    "print(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "print(f\"ProbeInterface version: {pi.__version__}\")\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-158-H10' #probe_name = 'ASSY-158-F' #probe_name = 'ASSY-158-H6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probe object from library comes with contact_ids and shank_ids info.\n",
    "probeH10 = pi.get_probe(manufacturer, probe_name)\n",
    "#Intan mapping 64 channels\n",
    "device_channel_indices = [24,23,25,22,26,21,27,20,28,19,29,18,30,17,31,16,0,15,1,14,2,13,3,12,4,11,5,10,6,9,7,8,\n",
    "                56,55,57,54,58,53,59,52,60,51,61,50,62,49,63,48,32,47,33,46,34,45,35,44,36,43,37,42,38,41,39,40] #Modify accordingly.\n",
    "#                88,87,89,86,90,85,91,84,92,83,93,82,94,81,95,80,64,79,65,78,66,77,67,76,68,75,69,74,70,73,71,72,\n",
    "#                120,119,121,118,122,117,123,116,124,115,125,114,126,113,127,112,96,111,97,110,98,109,99,108,100,107,101,106,102,105,103,104]\n",
    "#setting intan channels to probe\n",
    "probeH10.set_device_channel_indices(device_channel_indices) #print(probeH10.device_channel_indices)\n",
    "fig, ax = plt.subplots(figsize=(5, 10))\n",
    "plot_probe(probeH10, ax=ax, with_contact_id=True, with_device_index=True,)\n",
    "ax.set_xlim(-20, 200)\n",
    "ax.set_ylim(-60, 300)\n",
    "\n",
    "probeH10.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"shank_ids\", \"device_channel_indices\"]]\n",
    "#probeF64.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"shank_ids\", \"device_channel_indices\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe now is loaded with contact_ids, device_ids and shank_id.\n",
    "A probe (prb) or `probeinterface` object can be loaded directly to a SI recording object. A group can also be formed from each shank - 'by_shank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "recording_prb = recording.set_probe(probeH10, group_mode=\"by_probe\")\n",
    "print(recording_prb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = recording_prb.get_channel_ids()\n",
    "recording_slice = recording_prb.channel_slice(A[0:7]) #channel_ids = list(range(0, 4))\n",
    "si.plot_traces(recording_slice, segment_index=0, channel_ids=None,\n",
    "                          time_range=(0, 1.25), mode='line', backend='ipywidgets', \n",
    "                          show_channel_ids= True, clim=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n",
    "All preprocessing modules return new `RecordingExtractor` objects that apply the underlying preprocessing function. This allows users to access the preprocessed data in the same way as the raw data. We will focus only on the first shank (group `0`) for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recordings_by_group = recording_prb.split_by(\"group\")\n",
    "recording_to_process = recordings_by_group[0]\n",
    "recording_f = si.bandpass_filter(recording_to_process, freq_min=300, freq_max=6000)\n",
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "#recording_removeart = si.RemoveArtifactsRecording(recording_prb, list_triggers=triggers, ms_before=0.5, ms_after=3)\n",
    "recording_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#events0 = np.array([10.11, 10.17], [10.12, 10.18])\n",
    "#events = list(np.ndarray([[10, 17], [2, 20]]))#, dtype=float)]\n",
    "events = np.array([(10, 11), (12, 13)], dtype='f4, f4')\n",
    "#[[10.11, 10.17], [10.12, 10.18]])\n",
    "#events = event_list.tolist()\n",
    "channel_ids = list(range(0, 4))\n",
    "w = si.plot_traces({\"filtered\": recording_f, \"common\": recording_cmr}, mode='line',\n",
    "                   segment_index=0, channel_ids=channel_ids, show_channel_ids=True, events=events,\n",
    "                   time_range=[10, 11], backend='ipywidgets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = recording_slice.get_times(segment_index=0)\n",
    "last_time = times[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load start times from timestamps in CSV files\n",
    "for filename_st in os.listdir(data_folder):\n",
    "    if filename_st.startswith('Timestamps') and filename_st.endswith('.csv'):\n",
    "        csv_file = data_folder / filename_st\n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "with open(csv_file, 'r') as ft:\n",
    "        reader = csv.reader(ft)\n",
    "        time_str = next(reader)\n",
    "        start_time = \"\".join(time_str)\n",
    "        start_time = start_time[:15]\n",
    "        start_time = datetime.strptime(start_time, time_format)\n",
    "        \n",
    "#good for structured ttl, but start time get only from second file        \n",
    "# Step 2: Load events times from the CSV file, convert into seconds and structured np.array\n",
    "def load_event_times(csv_file):\n",
    "    event_times = []\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            time_str = row[0].strip()  # Ensure no leading/trailing spaces\n",
    "            event_times.append(time_str)\n",
    "    return event_times\n",
    "\n",
    "def convert_to_seconds(event_times):\n",
    "    #base_time = datetime.strptime(start_time, time_format)  # First event\n",
    "    events_seconds_array = []\n",
    "    for time_str in event_times:\n",
    "        # Limit to microseconds since datetime only supports up to 6 decimal places\n",
    "        current_time = datetime.strptime(time_str[:15], time_format)\n",
    "        delta_seconds = (current_time - start_time).total_seconds()\n",
    "        events_seconds_array.append(delta_seconds)\n",
    "    return np.array(events_seconds_array)\n",
    "\n",
    "\n",
    "def create_structured_array(events_seconds_array):\n",
    "    dtype = np.dtype([('time', np.float64)])  # Structured dtype\n",
    "    events_structured_array = np.zeros(len(events_seconds_array), dtype=dtype)\n",
    "    events_structured_array['time'] = events_seconds_array\n",
    "    return events_structured_array\n",
    "\n",
    "# Step 3: Load TTL times from the CSV file, convert into seconds and structured np.array\n",
    "def load_ttl_times(csv_file):\n",
    "    ttl_times = []\n",
    "    with open(csv_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            time_str = row[0].strip()  # Ensure no leading/trailing spaces\n",
    "            ttl_times.append(time_str)\n",
    "    return ttl_times\n",
    "\n",
    "def convert_to_seconds(ttl_times):\n",
    "    #base_time = datetime.strptime(ttl_times[0][:15], time_format)  # First event\n",
    "    ttl_seconds_array = []\n",
    "    for time_str in ttl_times:\n",
    "        # Limit to microseconds since datetime only supports up to 6 decimal places\n",
    "        current_time = datetime.strptime(time_str[:15], time_format)\n",
    "        delta_seconds = (current_time - start_time).total_seconds()\n",
    "        ttl_seconds_array.append(delta_seconds)\n",
    "    return np.array(ttl_seconds_array)\n",
    "\n",
    "def create_structured_array(ttl_seconds_array):\n",
    "    dtype = np.dtype([('time', np.float64)])  # Structured dtype\n",
    "    ttl_structured_array = np.zeros(len(ttl_seconds_array), dtype=dtype)\n",
    "    ttl_structured_array['time'] = ttl_seconds_array\n",
    "    return ttl_structured_array\n",
    "\n",
    "# Example usage\n",
    "for filename_ev in os.listdir(data_folder):\n",
    "    if filename_ev.startswith('Events') and filename_ev.endswith('.csv'):\n",
    "        csv_file = data_folder / filename_ev\n",
    "event_times = load_event_times(csv_file)  # Load time events\n",
    "\n",
    "for filename_ttl in os.listdir(data_folder):\n",
    "    if filename_ttl.startswith('TTL') and filename_ttl.endswith('.csv'):\n",
    "        csv_file = data_folder / filename_ttl\n",
    "ttl_times = load_ttl_times(csv_file)  # Load time TTL events\n",
    "\n",
    "#start_seconds_array = convert_to_seconds(start_times)  # Convert to seconds\n",
    "events_seconds_array = convert_to_seconds(event_times)  # Convert to seconds\n",
    "ttl_seconds_array = convert_to_seconds(ttl_times)  # Convert to seconds\n",
    "\n",
    "#start_structured_array = create_structured_array(start_seconds_array)  # Create structured array\n",
    "events_structured_array = create_structured_array(events_seconds_array)  # Create structured array\n",
    "ttl_structured_array = create_structured_array(ttl_seconds_array)  # Create structured array\n",
    "\n",
    "# Display the result\n",
    "#print(start_structured_array)\n",
    "print(events_structured_array)\n",
    "print(ttl_structured_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "#good for start time.\n",
    "# Define the directory where your CSV files are located\n",
    "csv_directory = data_folder  # Change this to the correct path\n",
    "time_format = \"%H:%M:%S.%f\"\n",
    "# Helper function to extract the first row from a CSV and convert to datetime\n",
    "def extract_start_time(file_path):\n",
    "    first_row = pd.read_csv(file_path, header=None).iloc[0, 0].strip()\n",
    "    return datetime.strptime(first_row[:15], time_format).time()\n",
    "\n",
    "# Find all timestamp files and extract their start times\n",
    "timestamp_files = sorted(glob.glob(os.path.join(csv_directory, \"Timestamps*.csv\")))\n",
    "start_times = [extract_start_time(file) for file in timestamp_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_datetime(time_str):\n",
    "        return datetime.strptime(time_str[:15], '%H:%M:%S.%f')\n",
    "\n",
    "start_times = []\n",
    "ttl_times = []\n",
    "event_times = []\n",
    "\n",
    "# Process Timestamps files\n",
    "for file in os.listdir(csv_directory):\n",
    "    if file.startswith('Timestamps') and file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(csv_directory, file), header=None)\n",
    "        first_row = df.iloc[0, 0]\n",
    "        start_time = time_to_datetime(first_row)\n",
    "        start_times.append(start_time)\n",
    "\n",
    "# Process TTL files\n",
    "for file in os.listdir(csv_directory):\n",
    "    if file.startswith('TTL') and file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(csv_directory, file), header=None)\n",
    "        ttl_time_list = df[0].apply(time_to_datetime).tolist()\n",
    "        ttl_times.append(ttl_time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract ttl time events from a CSV and convert to datetime\n",
    "def extract_ttl_time(file_path):\n",
    "    ttl_times = pd.read_csv(file_path, header=None).iloc[0, 0].strip()\n",
    "    return datetime.strptime(ttl_times[:15], time_format).time()\n",
    "#Good but extract only first row\n",
    "# Find all TTL files and extract their times\n",
    "ttl_files = sorted(glob.glob(os.path.join(csv_directory, \"TTL*.csv\")))\n",
    "ttl_times = [extract_ttl_time(file) for file in ttl_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate time differences and store in structured numpy array\n",
    "def process_event_file(file_path, start_time):\n",
    "    # Load the time events from the file\n",
    "    time_events = pd.read_csv(file_path, header=None)[0].str.strip()\n",
    "    \n",
    "    # Convert the time events to datetime objects with error handling\n",
    "    def parse_time(t):\n",
    "        try:\n",
    "            return datetime.strptime(t, \"%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            # Handle case where fractional seconds are missing\n",
    "            return datetime.strptime(t, \"%H:%M:%S\")\n",
    "\n",
    "    event_times = time_events.apply(parse_time)\n",
    "    \n",
    "    # Calculate the time differences in seconds\n",
    "    time_diffs = (event_times - start_time).dt.total_seconds()\n",
    "    \n",
    "    # Create a structured array to store the time differences\n",
    "    structured_array = np.zeros(len(time_diffs), dtype=[('time', 'f8')])\n",
    "    structured_array['time'] = time_diffs.values\n",
    "    return structured_array\n",
    "\n",
    "# Process Event and TTL files\n",
    "event_files = sorted(glob.glob(os.path.join(csv_directory, \"Events*.csv\")))\n",
    "ttl_files = sorted(glob.glob(os.path.join(csv_directory, \"TTL*.csv\")))\n",
    "\n",
    "# Ensure the number of timestamp files matches event and ttl files\n",
    "assert len(timestamp_files) == len(event_files) == len(ttl_files), \\\n",
    "    \"Mismatch in the number of timestamp, event, and TTL files.\"\n",
    "\n",
    "# Store structured arrays for events and TTLs\n",
    "events_structured_array = []\n",
    "ttl_structured_array = []\n",
    "\n",
    "for i in range(len(timestamp_files)):\n",
    "    # Process events and TTL files with corresponding start times\n",
    "    events_structured_array.append(process_event_file(event_files[i], start_times[i]))\n",
    "    ttl_structured_array.append(process_event_file(ttl_files[i], start_times[i]))\n",
    "\n",
    "# Example output (you can inspect the arrays or use them further)\n",
    "print(\"Events Structured Array:\")\n",
    "for arr in events_structured_array:\n",
    "    print(arr)\n",
    "\n",
    "print(\"\\nTTL Structured Array:\")\n",
    "for arr in ttl_structured_array:\n",
    "    print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains: .5943552",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 15\u001b[0m, in \u001b[0;36mextract_and_convert_csv_data.<locals>.convert_to_datetime\u001b[1;34m(time_str)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS.\u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# If nanoseconds are truncated, try without %f\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SI_env2024\\Lib\\_strptime.py:567\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03mformat string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SI_env2024\\Lib\\_strptime.py:352\u001b[0m, in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconverted data remains: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    353\u001b[0m                       data_string[found\u001b[38;5;241m.\u001b[39mend():])\n\u001b[0;32m    355\u001b[0m iso_year \u001b[38;5;241m=\u001b[39m year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: unconverted data remains: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[0;32m     46\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m data_folder  \u001b[38;5;66;03m# Update with your directory path\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m start_times, ttl_times, event_times \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_convert_csv_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Output the extracted data\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Times:\u001b[39m\u001b[38;5;124m\"\u001b[39m, start_times)\n",
      "Cell \u001b[1;32mIn[96], line 37\u001b[0m, in \u001b[0;36mextract_and_convert_csv_data\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvents\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Extract both columns as datetime objects\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 37\u001b[0m         event_time_pairs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_to_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     41\u001b[0m         event_times\u001b[38;5;241m.\u001b[39mappend(event_time_pairs)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_times, ttl_times, event_times\n",
      "Cell \u001b[1;32mIn[96], line 38\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvents\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# Extract both columns as datetime objects\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename), header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m         event_time_pairs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 38\u001b[0m             (\u001b[43mconvert_to_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, convert_to_datetime(row[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     40\u001b[0m         ]\n\u001b[0;32m     41\u001b[0m         event_times\u001b[38;5;241m.\u001b[39mappend(event_time_pairs)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_times, ttl_times, event_times\n",
      "Cell \u001b[1;32mIn[96], line 18\u001b[0m, in \u001b[0;36mextract_and_convert_csv_data.<locals>.convert_to_datetime\u001b[1;34m(time_str)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mstrptime(time_str, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# If nanoseconds are truncated, try without %f\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SI_env2024\\Lib\\_strptime.py:567\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m    569\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\SI_env2024\\Lib\\_strptime.py:352\u001b[0m, in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime data \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match format \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    350\u001b[0m                      (data_string, \u001b[38;5;28mformat\u001b[39m))\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconverted data remains: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    353\u001b[0m                       data_string[found\u001b[38;5;241m.\u001b[39mend():])\n\u001b[0;32m    355\u001b[0m iso_year \u001b[38;5;241m=\u001b[39m year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m month \u001b[38;5;241m=\u001b[39m day \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: unconverted data remains: .5943552"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_and_convert_csv_data(directory):\n",
    "    # Initialize lists to store datetime objects\n",
    "    start_times = []\n",
    "    ttl_times = []\n",
    "    event_times = []\n",
    "\n",
    "    # Helper function to clean and convert time strings\n",
    "    def convert_to_datetime(time_str):\n",
    "        time_str = time_str.strip()  # Remove leading/trailing whitespace\n",
    "        try:\n",
    "            return datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "        except ValueError:\n",
    "            # If nanoseconds are truncated, try without %f\n",
    "            return datetime.strptime(time_str, \"%H:%M:%S\")\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith(\"Timestamps\"):\n",
    "            # Extract the first row as a datetime object\n",
    "            df = pd.read_csv(os.path.join(directory, filename), header=None)\n",
    "            first_time = convert_to_datetime(df.iloc[0, 0])\n",
    "            start_times.append(first_time)\n",
    "\n",
    "        elif filename.startswith(\"TTL\"):\n",
    "            # Extract all rows as datetime objects\n",
    "            df = pd.read_csv(os.path.join(directory, filename), header=None)\n",
    "            times = [convert_to_datetime(time) for time in df[0]]\n",
    "            ttl_times.append(times)\n",
    "\n",
    "        elif filename.startswith(\"Events\"):\n",
    "            # Extract both columns as datetime objects\n",
    "            df = pd.read_csv(os.path.join(directory, filename), header=None)\n",
    "            event_time_pairs = [\n",
    "                (convert_to_datetime(row[0]), convert_to_datetime(row[1]))\n",
    "                for row in df.values\n",
    "            ]\n",
    "            event_times.append(event_time_pairs)\n",
    "\n",
    "    return start_times, ttl_times, event_times\n",
    "\n",
    "# Usage example\n",
    "directory_path = data_folder  # Update with your directory path\n",
    "start_times, ttl_times, event_times = extract_and_convert_csv_data(directory_path)\n",
    "\n",
    "# Output the extracted data\n",
    "print(\"Start Times:\", start_times)\n",
    "print(\"TTL Times:\", ttl_times)\n",
    "print(\"Event Times:\", event_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the directory containing your CSV files\n",
    "directory = data_folder\n",
    "\n",
    "# Function to convert time strings to datetime objects, handling extra digits\n",
    "def time_to_datetime(time_str):\n",
    "    try:\n",
    "        # Try parsing with standard microsecond precision\n",
    "        return datetime.strptime(time_str, '%H:%M:%S.%f')\n",
    "    except ValueError:\n",
    "        # Handle extra digits by truncating to 6 decimal places\n",
    "        truncated_time_str = time_str[:time_str.index('.') + 7]\n",
    "        return datetime.strptime(truncated_time_str, '%H:%M:%S.%f')\n",
    "\n",
    "# Initialize lists for storing data\n",
    "start_times = []\n",
    "ttl_times = []\n",
    "event_times = []\n",
    "\n",
    "# Process Timestamps files\n",
    "for file in os.listdir(directory):\n",
    "    if file.startswith('Timestamps') and file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(directory, file), header=None)\n",
    "        first_row = df.iloc[0, 0]\n",
    "        start_time = time_to_datetime(first_row)\n",
    "        start_times.append(start_time)\n",
    "\n",
    "# Process TTL files\n",
    "for file in os.listdir(directory):\n",
    "    if file.startswith('TTL') and file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(directory, file), header=None)\n",
    "        ttl_time_list = df[0].apply(time_to_datetime).tolist()\n",
    "        ttl_times.extend(ttl_time_list)\n",
    "\n",
    "# Process Events files\n",
    "for file in os.listdir(directory):\n",
    "    if file.startswith('Events') and file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(directory, file), header=None)\n",
    "        event_time_list = df.applymap(time_to_datetime).values.flatten().tolist()\n",
    "        event_times.extend(event_time_list)\n",
    "\n",
    "# Calculate time differences for TTL and Events relative to start times\n",
    "ttl_diff_in_seconds = [\n",
    "    (ttl_time - start_times[i % len(start_times)]).total_seconds()\n",
    "    for i, ttl_time in enumerate(ttl_times)\n",
    "]\n",
    "\n",
    "event_diff_in_seconds = [\n",
    "    (event_time - start_times[i % len(start_times)]).total_seconds()\n",
    "    for i, event_time in enumerate(event_times)\n",
    "]\n",
    "\n",
    "# Create structured numpy arrays\n",
    "ttl_structured_array = np.array(\n",
    "    [(time,) for time in ttl_diff_in_seconds], dtype=[('time', 'f8')]\n",
    ")\n",
    "\n",
    "events_structured_array = np.array(\n",
    "    [(time,) for time in event_diff_in_seconds], dtype=[('time', 'f8')]\n",
    ")\n",
    "\n",
    "# Optional: Display results for verification\n",
    "print(\"Start Times:\", start_times)\n",
    "print(\"TTL Structured Array:\")\n",
    "print(ttl_structured_array)\n",
    "print(\"Events Structured Array:\")\n",
    "print(events_structured_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = si.event.channel_ids\n",
    "num_channels = event.get_num_channels()\n",
    "# get structured dtype for the first channel\n",
    "event_dtype = event.get_dtype(channel_ids[0])\n",
    "print(event_dtype)\n",
    "# >>> dtype([('time', '<f8'), ('duration', '<f8'), ('label', '<U100')])\n",
    "\n",
    "# retrieve events (with structured dtype)\n",
    "events = event.get_events(channel_id=channel_ids[0], segment_index=0)\n",
    "# retrieve event times\n",
    "event_times = event.get_event_times(channel_id=channel_ids[0], segment_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only 5 min. for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we are going to spike sort the data, let's first cut out a 5-minute recording, to speed up computations.\n",
    "\n",
    "We can easily do so with the `frame_slice()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recording_sub = recording_cmr.frame_slice(start_frame=0*fs, end_frame=300*fs)\n",
    "print(recording_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Saving and loading SpikeInterface objects <a class=\"anchor\" id=\"save-load\"></a>\n",
    "\n",
    "All operations in SpikeInterface are *lazy*, meaning that they are not performed if not needed. This is why the creation of our filter recording was almost instantaneous. However, to speed up further processing, we might want to **save** it to a file and perform those operations (eg. filters, CMR, etc.) at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can use the si.set_global_job_kwargs() to set job_kwargs globally for the entire session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus - 2 #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "#global_job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "#si.set_global_job_kwargs(global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if (data_folder / \"preprocessed\").is_dir():\n",
    "    recording_saved = si.load_extractor(data_folder / \"preprocessed\")\n",
    "else:\n",
    "    recording_saved = recording_cmr.save(folder=data_folder / \"preprocessed\", **job_kwargs)\n",
    "    \n",
    "print(recording_saved)\n",
    "print(f'Cached channels ids:\\n {recording_saved.get_channel_ids()}')\n",
    "print(f'Channel groups after caching:\\n {recording_saved.get_channel_groups()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `traces_cached_seg0.raw` contains the processed raw data, while the `.json` files include information on how to reload the binary file. The `provenance.json` includes the information of the recording before saving it to a binary file, and the `probe.json` represents the probe object. The `save` returns a new *cached* recording that has all the previously loaded information: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After saving the SI object, we can easily load it back in a new session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recording_loaded = si.load_extractor(data_folder/\"preprocessed\")\n",
    "print(f'Loaded channels ids: {recording_loaded.get_channel_ids()}')\n",
    "print(f'Channel groups after loading: {recording_loaded.get_channel_groups()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check that the traces are exactly the same as the `recording_saved` that we saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "w_saved = si.plot_timeseries(recording_saved, ax=axs[0])\n",
    "w_loaded = si.plot_timeseries(recording_loaded, ax=axs[1])\n",
    "axs[0].set_title(\"Saved\")\n",
    "axs[1].set_title(\"Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**IMPORTANT**: the same saving mechanisms are available also for all SortingExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spike sorting <a class=\"anchor\" id=\"spike-sorting\"></a>\n",
    "\n",
    "We can now run spike sorting on the above recording. We will use different spike sorters for this demonstration, to show how easy SpikeInterface makes it easy to interchengably run different sorters :)\n",
    "\n",
    "Let's first check the installed sorters in `SpikeInterface` to see if `tridesclous` is available. Then we can then check the `tridesclous` default parameters.\n",
    "We will sort the bandpass cached filtered recording the `recording_saved` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "si.installed_sorters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "default_KS4_params = si.get_default_sorter_params('kilosort4')\n",
    "# Parameters can be changed by single arguments: \n",
    "#default_KS4_params['Th_universal'] = 9\n",
    "#sorter_params = {'do_correction': False} #??\n",
    "pprint(default_KS4_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.run_sorter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run spike sorting on recording\n",
    "#sorter_params = {'do_correction': False}\n",
    "sorting_KS4 = si.run_sorter('kilosort4', recording_sub, \n",
    "                            output_folder=data_folder / 'results_KS4',\n",
    "                            docker_image=True, verbose=True)#, **sorter_params, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_KS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_saved_KS4 = sorting_KS4.save(folder=data_folder / \"sorting_KS4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_loaded_KS4 = si.load_extractor(data_folder / \"sorting_KS4\")\n",
    "sorting_loaded_KS4\n",
    "#sorting_KS4 = si.read_sorter_folder(data_folder/\"results_KS4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use `spikewidgets` functions for some quick visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_rs = si.plot_rasters(sorting_KS4, time_range=(0, 60), backend='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. SortingAnalyzer <a class=\"anchor\" id=\"sortinganalyzer\"></a>\n",
    "\n",
    "The core module uses `SortingAnalyzer` for postprocessing computation from paired recording-sorting objects. It retrieves waveforms, templates, spike amplitudes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "si.create_sorting_analyzer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = si.create_sorting_analyzer(sorting_KS4, recording_sub, folder=data_folder / \"sorting_analyzer_3m\", \n",
    "                              format=\"binary_folder\", sparse=True, overwrite=True, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Analyzer in specific format and loading it from saved\n",
    "#sa.save_as(format=\"zarr\",folder=data_folder / \"sorting_analyzer_3m\")\n",
    "#sa_bin = si.load_sorting_analyzer(folder=data_folder / \"sorting_analyzer_3m\")\n",
    "#sa_zarr = si.load_sorting_analyzer(folder=data_folder / \"sorting_analyzer_3m.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Postprocessing <a class=\"anchor\" id=\"postprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Extensions: PCA, waveforms, templates, spike amplitude, correlograms, etc.\n",
    "\n",
    "Let's move on to explore the postprocessing capabilities of the `postprocessing` module. Similarly to the `SortingAnalizer` object, the method 'compute` retrieve info on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_computable_extensions = sa.get_computable_extensions()\n",
    "print(all_computable_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each call will recompute and overwrite previous computations\n",
    "sa.compute(\"random_spikes\")#subsample to create a template\n",
    "wf = sa.compute(\"waveforms\", ms_before=1.5, ms_after=2.5)\n",
    "sa.compute(\"templates\")#from raw waveforms or random_spikes\n",
    "sa.compute(\"spike_amplitudes\", peak_sign=\"neg\")#based on templates\n",
    "sa.compute(\"noise_levels\")#per channel\n",
    "sa.compute(\"principal_components\", n_components=3, mode=\"by_channel_local\")\n",
    "sa.compute(\"correlograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "sa.compute(\"isi_histograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "sa.compute(\"spike_locations\")#need for drift metrics (drift_ptp, drift_std, drift_mad)\n",
    "sa.compute(\"unit_locations\", \"template_metrics\", \"quality_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions are generally saved in two ways: \n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"memory\")\n",
    "\n",
    "sorting_analyzer.save_as(folder=\"my_sorting_analyzer\")\n",
    "sorting_analyzer.compute(\"random_spikes\", save=True)\n",
    "\n",
    "Here the random_spikes extension is not saved. The sorting_analyzer is still saved in memory. The save_as method only made a snapshot of the sorting analyzer which is saved in a folder. This is useful when trying out different parameters and initially setting up your pipeline. If we wanted to save the extension we should have started with a non-memory sorting analyzer:\n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"binary_folder\", folder=\"my_sorting_analyzer\")\n",
    "sorting_analyzer.compute(\"random_spikes\", save=True)\n",
    "\n",
    "NOTE: We recommend choosing a workflow and sticking with it. Either keep everything on disk or keep everything in memory until you’d like to save. A mixture can lead to unexpected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Quality Metrics <a class=\"anchor\" id=\"qualitymetrics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitud cutoff (calculate the approximate fraction of missing spikes)\n",
    "#Need \"spike_amplitudes\"\n",
    "fraction_missing = si.compute_amplitude_cutoffs(sa, peak_sign=\"neg\")\n",
    "\n",
    "#Amplitud CV (coefficient of variation)\n",
    "#Need \"spike_amplitudes\" or \"amplitude_scalings\" pre-computed.\n",
    "amplitude_cv_median, amplitude_cv_range = si.compute_amplitude_cv_metrics(sa)\n",
    "#dicts: unit ids as keys, and amplitude_cv metrics as values.\n",
    "\n",
    "#Drift metrics\n",
    "#Need \"spike_locations\"\n",
    "drift_ptps, drift_stds, drift_mads = si.compute_drift_metrics(sa)\n",
    "#dicts: unit ids as keys, and drifts metrics as values.\n",
    "\n",
    "#Firing Range (outside of physiological range, might indicate noise contamination)\n",
    "firing_range = si.compute_firing_ranges(sa)\n",
    "#dict: unit IDs as keys, firing_range as values (in Hz).\n",
    "\n",
    "#Firing Rate (average number of spikes/sec within the recording)\n",
    "firing_rate = si.compute_firing_rates(sa)\n",
    "#dict or floats: unit IDs as keys, firing rates across segments as values (in Hz).\n",
    "\n",
    "#Inter-spike-interval (ISI) Violations (rate of refractory period violations)\n",
    "isi_violations_ratio, isi_violations_count = si.compute_isi_violations(sa, isi_threshold_ms=1.0) \n",
    "#dicts: unit ids as keys, and isi ratio viol and number of viol as values.\n",
    "\n",
    "#Presence Ratio (proportion of discrete time bins in which at least one spike occurred)\n",
    "presence_ratio = si.compute_presence_ratios(sa)\n",
    "#dict: unit IDs as keys, presence ratio (between 0 and 1) as values.\n",
    "#Close or > 0.9 = complete units.\n",
    "#Close to 0 = incompleteness (type II error) or highly selective firing pattern.\n",
    "\n",
    "#Standard Deviation (SD) ratio\n",
    "sd_ratio = si.compute_sd_ratio(sa, censored_period_ms=4.0)\n",
    "#Close to 1 = unit from single neuron.\n",
    "\n",
    "#Signal-to-noise ratio (SNR)\n",
    "SNRs = si.compute_snrs(sa)\n",
    "#dict: unit IDs as keys and their SNRs as values.\n",
    "#High SNR = likely to correspond to a neuron. Low SNR = unit contaminated.\n",
    "\n",
    "#Synchrony Metrics (characterize synchronous events within the same spike train and across different spike trains)\n",
    "synchrony = si.compute_synchrony_metrics(sa, synchrony_sizes=(2, 4, 8))\n",
    "#tuple of dicts with the synchrony metrics for each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Distance (distance from a cluster to the nearest other cluster)\n",
    "iso_distance = si.pca_metrics.mahalanobis_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns floats: iso_distance, l_ratio.\n",
    "\n",
    "#Nearest Neighbor Metrics (evaluate unit quality)\n",
    "si.pca_metrics.nearest_neighbors_metrics(all_pcs, all_labels, this_unit_id, max_spikes, n_neighbors)\n",
    "#Calculate unit contamination based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_isolation(sa)\n",
    "#Calculate unit isolation based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_noise_overlap(sa)\n",
    "#Calculate unit noise overlap based on NearestNeighbors search in PCA space.\n",
    "\n",
    "#D-prime (estimate the classification accuracy between two units)\n",
    "d_prime = si.lda_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns a float (larger in well separated clusters)\n",
    "\n",
    "#Silhouette score (ratio between the cohesiveness of a cluster and its separation from other clusters)\n",
    "simple_sil_score = si.simplified_silhouette_score(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#Close to 1 = good clustering. Close to -1 = poorly isolated cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward way to filter a pandas dataframe is via the `query`.\n",
    "We first define our query (make sure the names match the column names of the dataframe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "our_query = f\"amplitude_cutoff < {amp_cutoff_thresh} & isi_violations_ratio < {isi_viol_thresh}\"\n",
    "print(our_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and then we can use the query to select units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keep_units = qm.query(our_query)\n",
    "keep_unit_ids = keep_units.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_auto = sorting_SC2.select_units(keep_unit_ids)\n",
    "print(f\"Number of units before curation: {len(sorting_SC2.get_unit_ids())}\")\n",
    "print(f\"Number of units after curation: {len(sorting_auto.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Viewers <a class=\"anchor\" id=\"viewers\"></a>\n",
    "\n",
    "Let's check put the `spikeinterface-gui` to explore our spike sorting results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeInterface GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!sigui waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "sw.plot_unit_locations(we_all, backend=\"ipywidgets\")\n",
    "sw.plot_spike_locations(we_all, backend=\"ipywidgets\")\n",
    "sw.plot_amplitudes(we_all, backend=\"ipywidgets\")\n",
    "sw.plot_autocorrelograms(we_all, unit_ids=sorting_SC2.unit_ids[:4])\n",
    "sw.plot_crosscorrelograms(we_all, unit_ids=sorting_SC2.unit_ids[:4])\n",
    "sw.plot_unit_templates(we_all, backend=\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Summary - SortingView"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sortingview` backend requires an additional step to configure the transfer of the data to be plotted to the cloud. \n",
    "\n",
    "See documentation [here](https://spikeinterface.readthedocs.io/en/latest/module_widgets.html): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 11. Exporters <a class=\"anchor\" id=\"exporters\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Export to Phy for manual curation\n",
    "\n",
    "To perform manual curation we can export the data to [Phy](https://github.com/cortex-lab/phy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexp.export_to_phy(we_all, output_folder=base_folder / 'phy_SC2_RN', compute_pc_features=True,\n",
    "                   copy_binary=True, dtype='float32', compute_amplitudes=True, template_mode='median', verbose=True,**job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#sexp.export_to_phy(we_all, output_folder=base_folder / 'phy_SC2c', \n",
    "#                   **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "!phy template-gui phy_SC2_RN/params.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After curating the results we can reload it using the `PhySortingExtractor` and exclude the units that we labeled as `noise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_phy_curated = se.PhySortingExtractor(base_folder / 'phy_SC2_RN/', exclude_cluster_groups=['noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of units before curation: {len(sorting_SC2.get_unit_ids())}\")\n",
    "print(f\"Number of units after curation: {len(sorting_phy_curated.get_unit_ids())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SI_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
