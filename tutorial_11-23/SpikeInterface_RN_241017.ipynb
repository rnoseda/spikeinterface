{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeInterface v0.101.1 - Adapted by Rodrigo Noseda - October 2024\n",
    "\n",
    "SpikeInterface to analyze a multichannel dataset from Cambridge Neurotech Probes. \n",
    "The dataset is extracted using open-ephys DAQ and Bonsai-rx (in .bin).\n",
    "Event_timestamps need some work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preparation <a class=\"anchor\" id=\"preparation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "print(f\"SpikeInterface Version: {si.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "%matplotlib widget\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading recording and probe information <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Setting file paths and basic parameters\n",
    "base_folder = Path('D:/Ephys_C2DRG/')\n",
    "data_folder = Path(\"D:/Ephys_C2DRG/2023_9_19/\")\n",
    "#Pasted directly from explorer \"C:\\Users\\rodri\\Documents\\Bonsai-RN\\Bonsai_DataRN\\2023_3_21\\\"\n",
    "\n",
    "recording_paths_list = []\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.startswith('RawEphysData') and filename.endswith('.bin'):\n",
    "        recording_paths_list.append(data_folder / filename)\n",
    "print('Recording Files List:')\n",
    "print(recording_paths_list)\n",
    "\n",
    "# parameters associated to the bin format\n",
    "num_channels = 64 #must know apriori; modify in probe below accordingly.\n",
    "fs = 30000\n",
    "gain_to_uV = 0.195\n",
    "offset_to_uV = 0\n",
    "dtype = \"float32\"\n",
    "time_axis = 0     \n",
    "time_format = \"%H:%M:%S.%f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0: Duration: 1800.032 sec - Samples: 54000960 - Has time vector?: False\n",
      "Segment 1: Duration: 1535.448 sec - Samples: 46063440 - Has time vector?: False\n"
     ]
    }
   ],
   "source": [
    "#Extract and append recording segments to Baserecording object\n",
    "recordings_list = []\n",
    "rec = si.read_binary(recording_paths_list, num_chan=num_channels,sampling_frequency=fs,\n",
    "                           dtype=dtype, gain_to_uV=gain_to_uV, offset_to_uV=offset_to_uV, \n",
    "                           time_axis=time_axis, is_filtered=False)\n",
    "recordings_list.append(rec)\n",
    "#Appending segments of recordings (Better because concatenate breaks timeline!)\n",
    "recording = si.append_recordings(recordings_list)\n",
    "#print(recording)\n",
    "for i in range(recording.get_num_segments()):\n",
    "    s = recording.get_num_samples(segment_index=i)\n",
    "    d = recording.get_duration(segment_index=i)\n",
    "    t = recording.has_time_vector(segment_index=i)\n",
    "    print(f\"Segment {i}: Duration: {d} sec - Samples: {s} - Has time vector?: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[0.00000000e+00 3.33333333e-05 6.66666667e-05 ... 1.80003190e+03\n",
      " 1.80003193e+03 1.80003197e+03]\n",
      "True\n",
      "[3600.490496   3600.49052933 3600.49056267 ... 5135.938396   5135.93842933\n",
      " 5135.93846267]\n"
     ]
    }
   ],
   "source": [
    "#Load times from timestamps csv files and calculate start time in seconds.\n",
    "tms_files = sorted(glob.glob(os.path.join(data_folder, \"Timestamps*.csv\")))\n",
    "concatenated_start_times = pd.DataFrame()\n",
    "for tms_file in tms_files:\n",
    "    df = pd.read_csv(tms_file, header=None, nrows=1, names=['Start_Times'])#(usecols=[0], nrows=1)\n",
    "    df['Start_Times'] = df['Start_Times'].str.slice(0, 15)#first = df.head(1) #last = df.tail(1)\n",
    "    concatenated_start_times = pd.concat([concatenated_start_times, df], ignore_index=True)\n",
    "concatenated_start_times['Start_Times'] = pd.to_datetime(concatenated_start_times['Start_Times'])\n",
    "time_diff = concatenated_start_times['Start_Times'] - concatenated_start_times['Start_Times'].iloc[0]\n",
    "seconds_start = time_diff.dt.total_seconds()\n",
    "\n",
    "#Get and set time vector for recording segments\n",
    "for i in range(recording.get_num_segments()):\n",
    "    tms = recording.get_times(segment_index=i)\n",
    "    tms_temp = tms + seconds_start[i]\n",
    "    recording.set_times(tms_temp, segment_index=i, with_warning=True)\n",
    "print(recording.has_time_vector(segment_index=0))\n",
    "print(recording.get_times(segment_index=0))\n",
    "print(recording.has_time_vector(segment_index=1))\n",
    "print(recording.get_times(segment_index=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4092.822695,) (4092.893069,) (4092.97129 ,) (4093.055821,)\n",
      " (4093.13129 ,) (4093.227725,) (4093.326861,) (4093.424167,)\n",
      " (4093.525044,) (4093.639732,) (4093.736653,) (4093.833908,)\n",
      " (4093.934196,) (4094.032845,) (4094.13106 ,) (4094.230029,)\n",
      " (4094.344781,) (4094.441472,) (4094.538215,) (4094.637991,)\n",
      " (4094.735693,) (4094.837466,) (4094.950964,) (4095.049012,)\n",
      " (4095.146484,) (4095.246208,) (4095.34473 ,) (4095.447668,)\n",
      " (4095.556071,) (4095.655629,) (4095.755367,) (4095.852416,)\n",
      " (4130.346714,) (4131.343207,) (4132.337754,) (4133.339111,)\n",
      " (4134.349812,) (4135.346957,) (4136.342439,) (4137.353447,)\n",
      " (4138.352461,) (4139.345831,) (4140.34889 ,) (4141.358144,)\n",
      " (4142.356544,) (4143.349325,) (4144.362612,) (4145.361306,)\n",
      " (4146.357645,) (4147.357581,) (4148.368666,) (4149.36274 ,)\n",
      " (4150.357837,) (4151.371264,) (4152.370087,) (4153.36704 ,)\n",
      " (4154.358656,) (4155.374196,) (4156.370151,) (4157.368359,)\n",
      " (4158.364084,) (4159.379546,) (4174.038567,) (4175.032487,)\n",
      " (4176.03186 ,) (4177.040525,) (4178.0405  ,) (4179.034957,)\n",
      " (4180.0464  ,) (4181.048   ,) (4182.04402 ,) (4183.039616,)\n",
      " (4184.049959,) (4185.044967,) (4186.047258,) (4187.039373,)\n",
      " (4188.05202 ,) (4189.047604,) (4190.099802,) (4191.05842 ,)\n",
      " (4192.058125,) (4193.052647,) (4194.045274,) (4195.061965,)\n",
      " (4196.059431,) (4197.055936,) (4198.066304,) (4199.063079,)\n",
      " (4200.062733,) (4201.064589,) (4202.06834 ,) (4203.065037,)\n",
      " (4255.424154,) (4256.416768,) (4257.418983,) (4258.428864,)\n",
      " (4259.422541,) (4260.42002 ,) (4261.432692,) (4262.430285,)\n",
      " (4263.425805,) (4264.55986 ,) (4265.435418,) (4266.437095,)\n",
      " (4267.428864,) (4268.441472,) (4269.442138,) (4270.43721 ,)\n",
      " (4271.431988,) (4272.44425 ,) (4273.443956,) (4274.439258,)\n",
      " (4275.450509,) (4276.446848,) (4277.446669,) (4278.444698,)\n",
      " (4279.45577 ,) (4280.44864 ,) (4281.444263,) (4282.444903,)\n",
      " (4283.457088,) (4284.453684,) (4309.479053,) (4309.58048 ,)\n",
      " (4309.675648,) (4309.771367,) (4309.885965,) (4309.985588,)\n",
      " (4310.085069,) (4310.185037,) (4310.28201 ,) (4310.381786,)\n",
      " (4310.480308,) (4310.594522,) (4310.69248 ,) (4310.788967,)\n",
      " (4310.888807,) (4310.988621,) (4311.085799,) (4311.200397,)\n",
      " (4311.298381,) (4311.397927,) (4311.493888,) (4311.595175,)\n",
      " (4311.690112,) (4311.805876,) (4311.904231,) (4312.006004,)\n",
      " (4312.103591,) (4312.201485,) (4312.297869,) (4312.400525,)\n",
      " (4312.513908,) (4312.611559,) (4312.712013,) (4312.810253,)\n",
      " (4312.907328,) (4313.009626,) (4313.118567,) (4313.21897 ,)\n",
      " (4313.317325,) (4313.413824,) (4313.514804,) (4313.614183,)\n",
      " (4313.708493,) (4313.825165,) (4313.922727,) (4314.023924,)\n",
      " (4314.119015,) (4314.219815,) (4314.31913 ,) (4314.433588,)\n",
      " (4314.531815,) (4314.630644,) (4314.729012,) (4314.825255,)\n",
      " (4314.925005,) (4315.038964,) (4315.137408,) (4315.238144,)\n",
      " (4315.334925,) (4315.433767,) (4315.536576,) (4315.635456,)\n",
      " (4315.747213,) (4315.844788,) (4315.944845,) (4316.039604,)\n",
      " (4316.140685,) (4316.238541,) (4316.518272,) (4316.594804,)\n",
      " (4316.669108,) (4316.747098,) (4316.818061,) (4316.898432,)\n",
      " (4316.971264,) (4317.062784,) (4317.157312,) (4317.255296,)\n",
      " (4317.352589,) (4317.453364,) (4317.548532,) (4317.664999,)\n",
      " (4317.76201 ,) (4317.862733,) (4317.960141,) (4318.058496,)\n",
      " (4318.158784,) (4318.270938,) (4318.370343,) (4318.47008 ,)\n",
      " (4318.568551,) (4318.669684,) (4318.765402,) (4318.865536,)\n",
      " (4318.97682 ,) (4319.076301,) (4319.172698,) (4319.274548,)\n",
      " (4319.374042,) (4319.469709,) (4319.583936,) (4319.683341,)\n",
      " (4319.7797  ,) (4319.880717,) (4319.98368 ,) (4320.078477,)\n",
      " (4320.176333,) (4320.290432,) (4320.389223,) (4320.486247,)\n",
      " (4320.585485,) (4320.68544 ,) (4320.781671,) (4320.899098,)\n",
      " (4320.995405,) (4321.09554 ,) (4321.192256,) (4321.292903,)\n",
      " (4321.389901,) (4321.503642,) (4321.602215,) (4321.703335,)\n",
      " (4321.799604,) (4321.90208 ,) (4321.999079,) (4322.095335,)\n",
      " (4322.208743,) (4322.308775,) (4322.408768,) (4322.506381,)\n",
      " (4322.602893,) (4322.706343,) (4322.81545 ,) (4322.915482,)\n",
      " (4323.012084,) (4323.113012,) (4323.211776,) (4323.307956,)\n",
      " (4323.408487,) (4323.522368,) (4323.622055,) (4323.722125,)\n",
      " (4323.821978,) (4323.91794 ,) (4324.01897 ,) (4324.130087,)\n",
      " (4324.228212,) (4324.327796,) (4324.423053,) (4324.523968,)\n",
      " (4324.622516,) (4324.736256,) (4324.835546,) (4324.93545 ,)\n",
      " (4325.032781,) (4325.132301,) (4325.22985 ,) (4325.329408,)\n",
      " (4325.44128 ,) (4325.545408,) (4325.640372,) (4325.736128,)\n",
      " (4325.839488,) (4325.93792 ,) (4326.048308,) (4326.148775,)\n",
      " (4326.246605,) (4326.346548,) (4326.442932,) (4326.541351,)\n",
      " (4326.641063,) (4326.755444,) (4326.854874,) (4326.951604,)\n",
      " (4327.05353 ,) (4327.151463,) (4327.248884,) (4327.361959,)\n",
      " (4327.461658,) (4327.559514,) (4327.659892,) (4327.755572,)\n",
      " (4327.859098,) (4327.96809 ,) (4328.070362,) (4328.166874,)\n",
      " (4328.264602,) (4328.364199,) (4328.463348,) (4328.559488,)\n",
      " (4328.675162,) (4328.773108,) (4328.873882,) (4328.970061,)\n",
      " (4329.071578,) (4329.169882,) (4329.283162,) (4329.381133,)\n",
      " (4329.480359,) (4329.583655,) (4329.678426,) (4329.773351,)\n",
      " (4329.888359,) (4329.987252,) (4330.086208,) (4330.185511,)\n",
      " (4330.280999,) (4330.382285,) (4330.478106,) (4330.594919,)\n",
      " (4330.69417 ,) (4330.793088,) (4330.891853,) (4330.988813,)\n",
      " (4331.088615,) (4331.20073 ,) (4331.3024  ,) (4331.39872 ,)\n",
      " (4331.497511,) (4331.595418,) (4331.697063,) (4331.792832,)\n",
      " (4331.907124,) (4332.006772,) (4332.106266,) (4332.205261,)\n",
      " (4332.305447,) (4332.403892,) (4332.513997,) (4332.612276,)\n",
      " (4332.710848,) (4332.810752,) (4332.915648,) (4333.005645,)\n",
      " (4333.120512,) (4333.219469,) (4333.316647,) (4333.415975,)\n",
      " (4333.515828,) (4333.616628,) (4333.714714,) (4333.826855,)\n",
      " (4333.926516,) (4334.022682,) (4334.1264  ,) (4334.219354,)\n",
      " (4334.323124,) (4334.432999,) (4334.532928,) (4334.63186 ,)\n",
      " (4334.731981,) (4334.828583,) (4334.928269,) (4335.032359,)\n",
      " (4335.139508,) (4335.238964,) (4335.34185 ,) (4335.434868,)\n",
      " (4335.534887,) (4335.63273 ,) (4335.747495,) (4335.846004,)\n",
      " (4335.942426,) (4336.042548,) (4336.142029,) (4336.240244,)\n",
      " (4336.352679,) (4336.452583,) (4336.551668,) (4336.650292,)\n",
      " (4336.753012,) (4336.843252,) (4336.946842,) (4337.059648,)\n",
      " (4337.158605,) (4337.2581  ,) (4337.35369 ,) (4337.453364,)\n",
      " (4337.5536  ,) (4337.665293,) (4337.767463,) (4337.864436,)\n",
      " (4337.962279,) (4338.066855,) (4338.165325,) (4338.256064,)\n",
      " (4338.374592,) (4338.47378 ,) (4338.569306,) (4338.673997,)\n",
      " (4338.766887,) (4338.868864,) (4338.980276,) (4339.078413,)\n",
      " (4339.177101,) (4339.276762,) (4339.376397,) (4339.470887,)\n",
      " (5682.959962,) (5683.971751,) (5684.971175,) (5685.968052,)\n",
      " (5686.97874 ,) (5687.977652,) (5688.992538,) (5689.969472,)\n",
      " (5690.981812,) (5691.977549,) (5692.975309,) (5693.986253,)\n",
      " (5694.983028,) (5695.983719,) (5696.979584,) (5697.988212,)\n",
      " (5698.985216,) (5699.986228,) (5700.994253,) (5701.99488 ,)\n",
      " (5702.98953 ,) (5703.987047,) (5705.00041 ,) (5705.996608,)\n",
      " (5706.995213,) (5707.989415,) (5709.001268,) (5709.996084,)\n",
      " (5710.998631,) (5712.00585 ,) (5722.888359,) (5723.888538,)\n",
      " (5724.880295,) (5725.90034 ,) (5726.891469,) (5727.889114,)\n",
      " (5728.885799,) (5729.898484,) (5730.895706,) (5731.894605,)\n",
      " (5732.9029  ,) (5733.899546,) (5734.899226,) (5735.895668,)\n",
      " (5736.906189,) (5737.966119,) (5738.898048,) (5739.911719,)\n",
      " (5740.907623,) (5741.907648,) (5742.904935,) (5743.91474 ,)\n",
      " (5744.912282,) (5745.912525,) (5746.919821,) (5747.920333,)\n",
      " (5748.913472,) (5749.914048,) (5750.924532,) (5751.917978,)\n",
      " (5769.942887,) (5770.942592,) (5771.950631,) (5772.948327,)\n",
      " (5773.944372,) (5774.954829,) (5775.952884,) (5776.952077,)\n",
      " (5777.950554,) (5778.958695,) (5779.954893,) (5780.955021,)\n",
      " (5781.946304,) (5782.961549,) (5783.957466,) (5784.959245,)\n",
      " (5785.966554,) (5787.061645,) (5787.961332,) (5788.962458,)\n",
      " (5789.969818,) (5790.97234 ,) (5791.964596,) (5792.974938,)\n",
      " (5793.973287,) (5794.970893,) (5795.964711,) (5796.979994,)\n",
      " (5797.976935,) (5798.974516,) (5821.202663,) (5821.302464,)\n",
      " (5821.401242,) (5821.501402,) (5821.598823,) (5821.698048,)\n",
      " (5821.808436,) (5821.909364,) (5822.007181,) (5822.107802,)\n",
      " (5822.206951,) (5822.303616,) (5822.402842,) (5822.515956,)\n",
      " (5822.614093,) (5822.713882,) (5822.816372,) (5822.914074,)\n",
      " (5823.011917,) (5823.122381,) (5823.226778,) (5823.323815,)\n",
      " (5823.419687,) (5823.520308,) (5823.616448,) (5823.728628,)\n",
      " (5823.830644,) (5823.929332,) (5824.026496,) (5824.124788,)\n",
      " (5824.22624 ,) (5824.323623,) (5824.435559,) (5824.535847,)\n",
      " (5824.632845,) (5824.734362,) (5824.832474,) (5824.928141,)\n",
      " (5825.041114,) (5825.14313 ,) (5825.239284,) (5825.338688,)\n",
      " (5825.4368  ,) (5825.536781,) (5825.637479,) (5825.747879,)\n",
      " (5825.846669,) (5825.946407,) (5826.048397,) (5826.142055,)\n",
      " (5826.241063,) (5826.356685,) (5826.454234,) (5826.550682,)\n",
      " (5826.652877,) (5826.748122,) (5826.84928 ,) (5826.961408,)\n",
      " (5827.062989,) (5827.161191,) (5827.259597,) (5827.362932,)\n",
      " (5827.456487,) (5827.558068,) (5827.667316,) (5827.767578,)\n",
      " (5827.86432 ,) (5827.965952,) (5828.0645  ,) (5828.162765,)\n",
      " (5828.274304,) (5828.375783,) (5828.472372,) (5828.570368,)\n",
      " (5828.670144,) (5828.766285,) (5828.866151,) (5828.980288,)\n",
      " (5829.078925,) (5829.18354 ,) (5829.277223,) (5829.377255,)\n",
      " (5829.478042,) (5829.588173,) (5829.686221,) (5829.786189,)\n",
      " (5829.884404,) (5829.983028,) (5830.084634,) (5830.194176,)\n",
      " (5830.295348,) (5830.392333,) (5830.488576,) (5830.590375,)\n",
      " (5830.69376 ,) (5830.787943,) (5830.900877,) (5830.998324,)\n",
      " (5831.098432,) (5831.197504,) (5831.294055,) (5831.395431,)\n",
      " (5831.50642 ,) (5831.605389,) (5831.704704,) (5831.805901,)\n",
      " (5831.903104,) (5832.00169 ,) (5832.097293,) (5832.213056,)\n",
      " (5832.31337 ,) (5832.410765,) (5832.509799,) (5832.607668,)\n",
      " (5832.708058,) (5832.820519,) (5832.919744,) (5833.018663,)\n",
      " (5833.114061,) (5833.216986,) (5833.31497 ,) (5833.426138,)\n",
      " (5833.527335,) (5833.623207,) (5833.723008,) (5833.822695,)\n",
      " (5833.922445,) (5834.021095,) (5834.132621,) (5834.230464,)\n",
      " (5834.329332,) (5834.434087,) (5834.52448 ,) (5834.628378,)\n",
      " (5834.739162,) (5834.839117,) (5834.936525,) (5835.036391,)\n",
      " (5835.136103,) (5835.230592,) (5835.331392,) (5835.445786,)\n",
      " (5835.545831,) (5835.645594,) (5835.742336,) (5835.839309,)\n",
      " (5835.94057 ,) (5836.052148,) (5836.15378 ,) (5836.251072,)\n",
      " (5836.349876,) (5836.450688,) (5836.549415,) (5836.65906 ,)\n",
      " (5836.758759,) (5836.858253,) (5836.95369 ,) (5837.054746,)\n",
      " (5837.151604,) (5837.248717,) (5837.365312,) (5837.463335,)\n",
      " (5837.563149,) (5837.661722,) (5837.761332,) (5837.859712,)\n",
      " (5837.970791,) (5838.072103,) (5838.174644,) (5838.270068,)\n",
      " (5838.368192,) (5838.464064,) (5838.571213,) (5838.677172,)\n",
      " (5838.775335,) (5838.877018,) (5838.972788,) (5839.076212,)\n",
      " (5839.176282,) (5839.286055,) (5839.384461,) (5839.483175,)\n",
      " (5839.584397,) (5839.681895,) (5839.779879,) (5839.8917  ,)\n",
      " (5839.990887,) (5840.091533,) (5840.19154 ,) (5840.289319,)\n",
      " (5840.385421,) (5840.485428,) (5840.596685,) (5840.697165,)\n",
      " (5840.795239,) (5840.894068,) (5840.992026,) (5841.098394,)\n",
      " (5841.203444,) (5841.303578,) (5841.402855,) (5841.501594,)\n",
      " (5841.599744,) (5841.701952,) (5841.795904,) (5841.911348,)\n",
      " (5842.011098,) (5842.107162,) (5842.209613,) (5842.311348,)\n",
      " (5842.406503,) (5842.518247,) (5842.616576,) (5842.715892,)\n",
      " (5842.813466,) (5842.912116,) (5843.014247,) (5843.12329 ,)\n",
      " (5843.221888,) (5843.32288 ,) (5843.42016 ,) (5843.523175,)\n",
      " (5843.61778 ,) (5843.713024,) (5843.829645,) (5843.9312  ,)\n",
      " (5844.026714,) (5844.128589,) (5844.230554,) (5844.323866,)\n",
      " (5844.435405,) (5844.53929 ,) (5844.635879,) (5844.730778,)\n",
      " (5844.83314 ,) (5844.93138 ,) (5845.043559,) (5845.142324,)\n",
      " (5845.241152,) (5845.502541,) (5845.577088,) (5845.647872,)\n",
      " (5845.720269,) (5845.802215,) (5845.884903,) (5845.963264,)\n",
      " (5846.049908,) (5846.14208 ,) (5846.245671,) (5846.358413,)\n",
      " (5846.455194,) (5846.552359,) (5846.652314,) (5846.749095,)\n",
      " (5846.851559,) (5846.948736,) (5847.063412,) (5847.161152,)\n",
      " (5847.261044,) (5847.360026,) (5847.456832,) (5847.556557,)\n",
      " (5847.669671,) (5847.767578,) (5847.868647,) (5847.9653  ,)\n",
      " (5848.063348,) (5848.164058,) (5848.276596,) (5848.374592,)\n",
      " (5848.474996,) (5848.572148,) (5848.673856,) (5848.771239,)\n",
      " (5848.872192,) (5848.981325,) (5849.079808,) (5849.180122,)\n",
      " (5849.280756,) (5849.378164,) (5849.477722,) (5849.588109,)\n",
      " (5849.690189,) (5849.7872  ,) (5849.887373,) (5849.98217 ,)\n",
      " (5850.084634,) (5850.183924,) (5850.296308,) (5850.394266,)\n",
      " (5850.493645,) (5850.592141,) (5850.687962,) (5850.789773,)\n",
      " (5850.904026,) (5851.006388,) (5851.101031,) (5851.202509,)]\n"
     ]
    }
   ],
   "source": [
    "#Load ttl times from csv files and calculate time in seconds.\n",
    "ttl_files = sorted(glob.glob(os.path.join(data_folder, \"TTL*.csv\")))\n",
    "ttl_times = pd.DataFrame()\n",
    "dtype = np.dtype([('time', np.float64)])\n",
    "ttl_array = np.empty((0), dtype=dtype)\n",
    "ttl_array_list = np.empty((0), dtype=dtype)\n",
    "for ttl_file in ttl_files:\n",
    "    df = pd.read_csv(ttl_file, header=None, usecols=[0], names=['TTL_Times'])#(usecols=[0], nrows=1)\n",
    "    df['TTL_Times'] = df['TTL_Times'].str.slice(0, 15)#first = df.head(1) #last = df.tail(1)\n",
    "    df['TTL_Times'] = pd.to_datetime(df['TTL_Times'])\n",
    "    time_diff_ttl = df['TTL_Times'] - concatenated_start_times['Start_Times'].iloc[0]\n",
    "    seconds_ttl = time_diff_ttl.dt.total_seconds()\n",
    "    ttl_array = np.zeros(len(seconds_ttl), [('time', '<f8')])\n",
    "    ttl_array['time'] = seconds_ttl.to_list()\n",
    "ttl_array_list = np.append(ttl_array_list, ttl_array, axis=0)\n",
    "\n",
    "#concatenated_ttl_times = pd.concat([concatenated_ttl_times, df], ignore_index=True)\n",
    "\n",
    "print(ttl_array_list)\n",
    "\n",
    "#np.array(list(arr[:, 1]), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_folder / 'TTL0_0.csv', header=None, usecols=[0], names=['TTL_Times'])#(usecols=[0], nrows=1)\n",
    "df['TTL_Times'] = df['TTL_Times'].str.slice(0, 15)#first = df.head(1) #last = df.tail(1)\n",
    "df['TTL_Times'] = pd.to_datetime(df['TTL_Times'])\n",
    "time_diff_ttl = df['TTL_Times'] - concatenated_start_times['Start_Times'].iloc[0]\n",
    "seconds_ttl = time_diff_ttl.dt.total_seconds()\n",
    "#array = np.array(list(seconds_ttl), dtype=np.float64)\n",
    "array1 = np.array(list(seconds_ttl), dtype=[('time', 'f8')])\n",
    "array2 = np.array(list(seconds_ttl+300), dtype=[('time', 'f8')])\n",
    "#print(array1, array2)\n",
    "#data = np.array([(1.4,), (2.5,)], dtype=[('time', '<f8')])\n",
    "events = [array1, array2]\n",
    "#data = [(1.4, 1.5) (2.5, 2.6)]\n",
    "#dtype = [('time', '<f8')]\n",
    "\n",
    "#arr = np.array(data, dtype=dtype)\n",
    "#ttl_structured_array\n",
    "#arr = np.array([(time,) for time in data], dtype=[('time', 'f8')])\n",
    "#print(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_folder / 'TTL0_0.csv', header=None, usecols=[0], names=['TTL_Times'])#(usecols=[0], nrows=1)\n",
    "df['TTL_Times'] = df['TTL_Times'].str.slice(0, 15)#first = df.head(1) #last = df.tail(1)\n",
    "df['TTL_Times'] = pd.to_datetime(df['TTL_Times'])\n",
    "time_diff_ttl = df['TTL_Times'] - concatenated_start_times['Start_Times'].iloc[0]\n",
    "seconds_ttl = []\n",
    "seconds_ttl = time_diff_ttl.dt.total_seconds()\n",
    "tuple1 = tuple(seconds_ttl.to_list())\n",
    "tuple2 = tuple(seconds_ttl.to_list())\n",
    "#array = np.array(tuple1)\n",
    "seconds_ttl_2 = [tuple1, tuple2]\n",
    "dtype = [('time', '<f8')]\n",
    "array = np.array(seconds_ttl_2, dtype=np.float64)\n",
    "print(array)\n",
    "#print(tuple2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Get probe from library and set channel mapping\n",
    "import probeinterface as pi\n",
    "from probeinterface.plotting import plot_probe\n",
    "print(f\"ProbeInterface version: {pi.__version__}\")\n",
    "manufacturer = 'cambridgeneurotech'\n",
    "probe_name = 'ASSY-158-H10' #probe_name = 'ASSY-158-F' #probe_name = 'ASSY-158-H6'\n",
    "probeH10 = pi.get_probe(manufacturer, probe_name)#library: comes with contact_ids and shank_ids info.\n",
    "\n",
    "#Mapping Intan (device) channels\n",
    "device_channel_indices = [24,23,25,22,26,21,27,20,28,19,29,18,30,17,31,16,0,15,1,14,2,13,3,12,4,11,5,10,6,9,7,8,\n",
    "    56,55,57,54,58,53,59,52,60,51,61,50,62,49,63,48,32,47,33,46,34,45,35,44,36,43,37,42,38,41,39,40] #Modify accordingly.\n",
    "#   88,87,89,86,90,85,91,84,92,83,93,82,94,81,95,80,64,79,65,78,66,77,67,76,68,75,69,74,70,73,71,72,\n",
    "#   120,119,121,118,122,117,123,116,124,115,125,114,126,113,127,112,96,111,97,110,98,109,99,108,100,107,101,106,102,105,103,104]\n",
    "#Setting Intan channels to probe(RHD-2132/2164)\n",
    "probeH10.set_device_channel_indices(device_channel_indices) #print(probeH10.device_channel_indices)\n",
    "fig, ax = plt.subplots(figsize=(5, 10))\n",
    "plot_probe(probeH10, ax=ax, with_contact_id=True, with_device_index=True,)\n",
    "ax.set_xlim(-20, 200)\n",
    "ax.set_ylim(-60, 300)\n",
    "#probeH10.to_dataframe(complete=True).loc[:, [\"contact_ids\", \"shank_ids\", \"device_channel_indices\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe now is loaded with contact_ids, device_ids and shank_id.\n",
    "A probe (prb) or `probeinterface` object can be loaded directly to a SI recording object. A group can also be formed from each probe ('by_probe') or shank ('by_shank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recording_prb = recording.set_probe(probeH10, group_mode=\"by_probe\")\n",
    "channels_ids = recording_prb.get_channel_ids()\n",
    "recording_slice = recording_prb.channel_slice(channel_ids=channels_ids[:3]) #channel_ids = list(range(0, 4))\n",
    "si.plot_traces(recording_slice, segment_index=0, channel_ids=None,\n",
    "                          time_range=(0, 2.25), mode='line', backend='ipywidgets', \n",
    "                          show_channel_ids= True, clim=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>\n",
    "\n",
    "All preprocessing modules return new `RecordingExtractor` objects that apply the underlying preprocessing function. This allows users to access the preprocessed data in the same way as the raw data. We will focus only on the first shank (group `0`) for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recordings_by_group = recording_prb.split_by(\"group\")\n",
    "recording_to_process = recordings_by_group[0]\n",
    "recording_f = si.bandpass_filter(recording_to_process, freq_min=300, freq_max=6000)\n",
    "recording_cmr = si.common_reference(recording_f, reference='global', operator='median')\n",
    "#recording_removeart = si.RemoveArtifactsRecording(recording_prb, list_triggers=triggers, ms_before=0.5, ms_after=3)\n",
    "recording_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load times from timestamps csv files\n",
    "tms_files = sorted(glob.glob(os.path.join(data_folder, \"Timestamps*.csv\")))\n",
    "concatenated_first_last = pd.DataFrame()\n",
    "for tms_file in tms_files:\n",
    "    df = pd.read_csv(tms_file, header=None, names=['Timestamps'])#(usecols=[0], nrows=1)\n",
    "    df['Timestamps'] = df['Timestamps'].str.slice(0, 15)\n",
    "    first = df.head(1)\n",
    "    last = df.tail(1)\n",
    "    concatenated_first_last = pd.concat([concatenated_first_last, first, last], ignore_index=True)\n",
    "#print(concatenated_first_last)\n",
    "\n",
    "# Step 2: Process TTLs from csv files\n",
    "ttl_files = sorted(glob.glob(os.path.join(data_folder, \"TTL*.csv\")))\n",
    "concatenated_df_ttl = pd.DataFrame()\n",
    "for ttl_file in ttl_files:\n",
    "    df = pd.read_csv(ttl_file, header=None, names=['Timestamps'])#(usecols=[0], nrows=1)\n",
    "    df['Timestamps'] = df['Timestamps'].str.slice(0, 15)\n",
    "    concatenated_df_ttl = pd.concat([concatenated_df_ttl, df], ignore_index=True)\n",
    "#print(concatenated_df_ttl)\n",
    "\n",
    "# Step 3: Process Events from csv files\n",
    "events_files = sorted(glob.glob(os.path.join(data_folder, \"Events*.csv\")))\n",
    "concatenated_df_events = pd.DataFrame()\n",
    "for event_file in events_files:\n",
    "    df = pd.read_csv(event_file, usecols=[0, 1], header=None, names=['Timestamps_Start', 'Timestamps_Stop'])\n",
    "    df['Timestamps_Start'] = df['Timestamps_Start'].str.slice(0, 15)\n",
    "    df['Timestamps_Stop'] = df['Timestamps_Stop'].str.slice(0, 15)\n",
    "    concatenated_df_events = pd.concat([concatenated_df_events, df], ignore_index=True)\n",
    "#print(concatenated_df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert timestamps to datetime objects.\n",
    "concatenated_first_last['Timestamps'] = pd.to_datetime(concatenated_first_last['Timestamps'])\n",
    "concatenated_df_ttl['Timestamps'] = pd.to_datetime(concatenated_df_ttl['Timestamps'])\n",
    "concatenated_df_events['Timestamps_Start'] = pd.to_datetime(concatenated_df_events['Timestamps_Start'])\n",
    "concatenated_df_events['Timestamps_Stop'] = pd.to_datetime(concatenated_df_events['Timestamps_Stop'])\n",
    "\n",
    "# Step 5: Calculate total seconds of ttl and events from start time.\n",
    "time_diff = concatenated_df_ttl['Timestamps'] - concatenated_first_last.loc[0, 'Timestamps']\n",
    "time_diff2 = concatenated_df_events['Timestamps_Start'] - concatenated_first_last.loc[0, 'Timestamps']\n",
    "time_diff3 = concatenated_df_events['Timestamps_Stop'] - concatenated_first_last.loc[0, 'Timestamps']\n",
    "\n",
    "seconds_ttl = time_diff.dt.total_seconds()\n",
    "seconds_events_start = time_diff2.dt.total_seconds()\n",
    "seconds_events_stop = time_diff3.dt.total_seconds()\n",
    "#print(seconds_ttl, seconds_events_start, seconds_events_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Convert times in seconds to structured numpy arrays.\n",
    "np.array(seconds_ttl)\n",
    "def create_structured_array(seconds_ttl):\n",
    "    dtype = np.dtype([('time', np.float64)])  # Structured dtype\n",
    "    ttl_structured_array = np.zeros(len(seconds_ttl), dtype=dtype)\n",
    "    ttl_structured_array['time'] = seconds_ttl\n",
    "    return ttl_structured_array\n",
    "ttl_structured_array = create_structured_array(seconds_ttl)  # Create structured array\n",
    "\n",
    "# Display the result\n",
    "print(ttl_structured_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe41d23b498c4d99b1df977b87916166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(TimeSlider(children=(Dropdown(description='segment', options=(0, 1), value=0), Button(icon…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#events0 = np.array([10.11, 10.17], [10.12, 10.18])\n",
    "#events = list(np.ndarray([[10, 17], [2, 20]]))#, dtype=float)]\n",
    "#dtype = [('time', 'float64')]\n",
    "#events = np.array([(10.1), (10.7)])#, dtype=dtype)#np.float64)#[('time', 'f4')])\n",
    "#events = array\n",
    " \n",
    "# Create structured numpy arrays\n",
    "#ttl_structured_array = np.array([(time,) for time in ttl_diff_in_seconds], dtype=[('time', 'f8')])\n",
    "channel_ids = list(range(0, 2))\n",
    "w = si.plot_traces({\"filtered\": recording_f, \"common\": recording_cmr}, mode='line',\n",
    "                   segment_index=0, channel_ids=channel_ids, show_channel_ids=True, events=events,\n",
    "                   time_range=[9, 18], backend='ipywidgets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time differences for TTL and Events relative to start times\n",
    "ttl_diff_in_seconds = [(ttl_time - start_times[i % len(start_times)]).total_seconds()\n",
    "    for i, ttl_time in enumerate(ttl_times)]\n",
    "\n",
    "event_diff_in_seconds = [(event_time - start_times[i % len(start_times)]).total_seconds()\n",
    "    for i, event_time in enumerate(event_times)]\n",
    "\n",
    "# Create structured numpy arrays\n",
    "ttl_structured_array = np.array([(time,) for time in ttl_diff_in_seconds], dtype=[('time', 'f8')])\n",
    "events_structured_array = np.array([(time,) for time in event_diff_in_seconds], dtype=[('time', 'f8')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take only 5 min. for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we are going to spike sort the data, let's first cut out a 5-minute recording, to speed up computations.\n",
    "\n",
    "We can easily do so with the `frame_slice()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recording_select = si.select_segment_recording(recording_cmr, segment_indices=0)\n",
    "recording_sub = recording_select.frame_slice(start_frame=0*fs, end_frame=300*fs)\n",
    "print(recording_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Saving and loading SpikeInterface objects <a class=\"anchor\" id=\"save-load\"></a>\n",
    "\n",
    "All operations in SpikeInterface are *lazy*, meaning that they are not performed if not needed. This is why the creation of our filter recording was almost instantaneous. However, to speed up further processing, we might want to **save** it to a file and perform those operations (eg. filters, CMR, etc.) at once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can use the si.set_global_job_kwargs() to set job_kwargs globally for the entire session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "n_cpus = os.cpu_count()\n",
    "n_jobs = n_cpus# - 2 #n_jobs = -1 :equal to the number of cores.\n",
    "job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "#global_job_kwargs = dict(n_jobs=n_jobs, chunk_duration=\"1s\", progress_bar=True)\n",
    "#si.set_global_job_kwargs(global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if (data_folder / \"preprocessed\").is_dir():\n",
    "    recording_saved = si.load_extractor(data_folder / \"preprocessed\")\n",
    "else:\n",
    "    recording_saved = recording_cmr.save(folder=data_folder / \"preprocessed\", **job_kwargs)\n",
    "    \n",
    "print(recording_saved)\n",
    "print(f'Cached channels ids:\\n {recording_saved.get_channel_ids()}')\n",
    "print(f'Channel groups after caching:\\n {recording_saved.get_channel_groups()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `traces_cached_seg0.raw` contains the processed raw data, while the `.json` files include information on how to reload the binary file. The `provenance.json` includes the information of the recording before saving it to a binary file, and the `probe.json` represents the probe object. The `save` returns a new *cached* recording that has all the previously loaded information: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After saving the SI object, we can easily load it back in a new session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recording_loaded = si.load_extractor(data_folder/\"preprocessed\")\n",
    "print(f'Loaded channels ids: {recording_loaded.get_channel_ids()}')\n",
    "print(f'Channel groups after loading: {recording_loaded.get_channel_groups()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check that the traces are exactly the same as the `recording_saved` that we saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2)\n",
    "w_saved = si.plot_timeseries(recording_saved, ax=axs[0])\n",
    "w_loaded = si.plot_timeseries(recording_loaded, ax=axs[1])\n",
    "axs[0].set_title(\"Saved\")\n",
    "axs[1].set_title(\"Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**IMPORTANT**: the same saving mechanisms are available also for all SortingExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Spike sorting <a class=\"anchor\" id=\"spike-sorting\"></a>\n",
    "\n",
    "We can now run spike sorting on the above recording. We will use different spike sorters for this demonstration, to show how easy SpikeInterface makes it easy to interchengably run different sorters :)\n",
    "\n",
    "Let's first check the installed sorters in `SpikeInterface` to see if `tridesclous` is available. Then we can then check the `tridesclous` default parameters.\n",
    "We will sort the bandpass cached filtered recording the `recording_saved` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "si.installed_sorters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "default_KS4_params = si.get_default_sorter_params('kilosort4')\n",
    "# Parameters can be changed by single arguments: \n",
    "#default_KS4_params['Th_universal'] = 9\n",
    "#sorter_params = {'do_correction': False} #??\n",
    "pprint(default_KS4_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.run_sorter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run spike sorting on recording using docker container\n",
    "sorting_KS4 = si.run_sorter('kilosort4', recording_sub, \n",
    "                            output_folder=data_folder / 'results_KS4',\n",
    "                            docker_image=True, verbose=True)#, **sorter_params, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_KS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_saved_KS4 = sorting_KS4.save(folder=data_folder / \"sorting_KS4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_loaded_KS4 = si.load_extractor(data_folder / \"sorting_KS4\")\n",
    "sorting_loaded_KS4\n",
    "#sorting_KS4 = si.read_sorter_folder(data_folder/\"results_KS4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use `spikewidgets` functions for some quick visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "w_rs = si.plot_rasters(sorting_KS4, time_range=(0, 300), backend='matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Postprocessing: SortingAnalyzer <a class=\"anchor\" id=\"sortinganalyzer\"></a>\n",
    "\n",
    "The core module uses `SortingAnalyzer` for postprocessing computation from paired recording-sorting objects. It retrieves waveforms, templates, spike amplitudes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparsity = si.estimate_sparsity?\n",
    "sparsity = si.estimate_sparsity(sorting_KS4,recording, num_spikes_for_sparsity=200, method=\"radius\",\n",
    "                                radius_um=40, peak_sign=\"both\", amplitude_mode=\"extremum\")\n",
    "#sparsity2 = si.compute_sparsity(sorting_KS4,recording)\n",
    "for unit_id in sparsity.unit_ids[::30]:\n",
    "    print(unit_id, list(sparsity.unit_id_to_channel_ids[unit_id]))\n",
    "#most of the plotting, computation and export functions are using 'sparsity' in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si.create_sorting_analyzer?\n",
    "sa = si.create_sorting_analyzer(sorting_KS4, recording_sub, folder=data_folder / \"sorting_analyzer\", \n",
    "                              format=\"binary_folder\", sparsity=sparsity, overwrite=True, **job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Analyzer in specific format and loading it from saved\n",
    "#sa.save_as(format=\"zarr\",folder=data_folder / \"sorting_analyzer\")\n",
    "#sa_bin = si.load_sorting_analyzer(folder=data_folder / \"sorting_analyzer\")\n",
    "#sa_zarr = si.load_sorting_analyzer(folder=data_folder / \"sorting_analyzer.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Extensions: PCA, waveforms, templates, spike amplitude, correlograms, etc.\n",
    "\n",
    "Let's move on to explore the postprocessing capabilities of the `postprocessing` module. Similarly to the `SortingAnalizer` object, the method 'compute` retrieve info on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_computable_extensions = sa.get_computable_extensions()\n",
    "print(all_computable_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SortingAnalizer computations: each call will recompute and overwrite previous computations\n",
    "rand = sa.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)#subsample to create a template\n",
    "wf = sa.compute(\"waveforms\", ms_before=1, ms_after=2, **job_kwargs)\n",
    "templ =sa.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])#from raw waveforms or random_spikes\n",
    "spk_amp = sa.compute(\"spike_amplitudes\", peak_sign=\"neg\")#based on templates\n",
    "noise = sa.compute(\"noise_levels\")\n",
    "amp_scal = sa.compute(\"amplitude_scalings\")#per channel\n",
    "pca = sa.compute(\"principal_components\", n_components=3, mode=\"by_channel_local\")\n",
    "corr = sa.compute(\"correlograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "isi = sa.compute(\"isi_histograms\", window_ms=50.0, bin_ms=1.0, method=\"auto\")\n",
    "spk_loc = sa.compute(\"spike_locations\", method=\"center_of_mass\")#need for drift metrics (drift_ptp, drift_std, drift_mad)\n",
    "templ_sim = sa.compute(\"template_similarity\")#need for spikeinterface_gui\n",
    "u_loc = sa.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "templ_metric = sa.compute(\"template_metrics\")\n",
    "qm = sa.compute(\"quality_metrics\")\n",
    "# qm = si.compute_quality_metrics(analyzer, metric_names=metric_names) #display(qm) #using 'si' gives a dataframe.\n",
    "#sparsity3 = sa.compute(\"sparsity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_ext = sa.get_extension(\"waveforms\")\n",
    "wf0 = wf_ext.get_waveforms_one_unit(unit_id=0)\n",
    "print(f\"Waveform shape: {wf0.shape}\")\n",
    "\n",
    "templ_ext = sa.get_extension(\"templates\")\n",
    "templ0 = templ_ext.get_templates(operator=\"average\")\n",
    "print(type(templ0), templ0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extensions are generally saved in two ways: \n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"memory\")\n",
    "\n",
    "sorting_analyzer.save_as(folder=\"my_sorting_analyzer\")\n",
    "sorting_analyzer.compute(\"random_spikes\", save=True)\n",
    "\n",
    "Here the random_spikes extension is not saved. The sorting_analyzer is still saved in memory. The save_as method only made a snapshot of the sorting analyzer which is saved in a folder. This is useful when trying out different parameters and initially setting up your pipeline. If we wanted to save the extension we should have started with a non-memory sorting analyzer:\n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format=\"binary_folder\", folder=\"my_sorting_analyzer\")\n",
    "sorting_analyzer.compute(\"random_spikes\", save=True)\n",
    "\n",
    "NOTE: We recommend choosing a workflow and sticking with it. Either keep everything on disk or keep everything in memory until you’d like to save. A mixture can lead to unexpected behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Quality Metrics <a class=\"anchor\" id=\"qualitymetrics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.get_default_qm_params()\n",
    "si.get_quality_metric_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amplitud cutoff (calculate the approximate fraction of missing spikes)\n",
    "#Need \"spike_amplitudes\"\n",
    "fraction_missing = si.compute_amplitude_cutoffs(sa, peak_sign=\"neg\")\n",
    "\n",
    "#Amplitud CV (coefficient of variation)\n",
    "#Need \"spike_amplitudes\" or \"amplitude_scalings\" pre-computed.\n",
    "amplitude_cv_median, amplitude_cv_range = si.compute_amplitude_cv_metrics(sa)\n",
    "#dicts: unit ids as keys, and amplitude_cv metrics as values.\n",
    "\n",
    "#Drift metrics\n",
    "#Need \"spike_locations\"\n",
    "drift_ptps, drift_stds, drift_mads = si.compute_drift_metrics(sa)\n",
    "#dicts: unit ids as keys, and drifts metrics as values.\n",
    "\n",
    "#Firing Range (outside of physiological range, might indicate noise contamination)\n",
    "firing_range = si.compute_firing_ranges(sa)\n",
    "#dict: unit IDs as keys, firing_range as values (in Hz).\n",
    "\n",
    "#Firing Rate (average number of spikes/sec within the recording)\n",
    "firing_rate = si.compute_firing_rates(sa)\n",
    "#dict or floats: unit IDs as keys, firing rates across segments as values (in Hz).\n",
    "\n",
    "#Inter-spike-interval (ISI) Violations (rate of refractory period violations)\n",
    "isi_violations_ratio, isi_violations_count = si.compute_isi_violations(sa, isi_threshold_ms=1.0) \n",
    "#dicts: unit ids as keys, and isi ratio viol and number of viol as values.\n",
    "\n",
    "#Presence Ratio (proportion of discrete time bins in which at least one spike occurred)\n",
    "presence_ratio = si.compute_presence_ratios(sa)\n",
    "#dict: unit IDs as keys, presence ratio (between 0 and 1) as values.\n",
    "#Close or > 0.9 = complete units.\n",
    "#Close to 0 = incompleteness (type II error) or highly selective firing pattern.\n",
    "\n",
    "#Standard Deviation (SD) ratio\n",
    "sd_ratio = si.compute_sd_ratio(sa, censored_period_ms=4.0)\n",
    "#Close to 1 = unit from single neuron.\n",
    "\n",
    "#Signal-to-noise ratio (SNR)\n",
    "SNRs = si.compute_snrs(sa)\n",
    "#dict: unit IDs as keys and their SNRs as values.\n",
    "#High SNR = likely to correspond to a neuron. Low SNR = unit contaminated.\n",
    "\n",
    "#Synchrony Metrics (characterize synchronous events within the same spike train and across different spike trains)\n",
    "synchrony = si.compute_synchrony_metrics(sa, synchrony_sizes=(2, 4, 8))\n",
    "#tuple of dicts with the synchrony metrics for each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.get_quality_pca_metric_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolation Distance (distance from a cluster to the nearest other cluster)\n",
    "iso_distance = si.pca_metrics.mahalanobis_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns floats: iso_distance, l_ratio.\n",
    "\n",
    "#Nearest Neighbor Metrics (evaluate unit quality)\n",
    "si.pca_metrics.nearest_neighbors_metrics(all_pcs, all_labels, this_unit_id, max_spikes, n_neighbors)\n",
    "#Calculate unit contamination based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_isolation(sa)\n",
    "#Calculate unit isolation based on NearestNeighbors search in PCA space.\n",
    "si.pca_metrics.nearest_neighbors_noise_overlap(sa)\n",
    "#Calculate unit noise overlap based on NearestNeighbors search in PCA space.\n",
    "\n",
    "#D-prime (estimate the classification accuracy between two units)\n",
    "d_prime = si.lda_metrics(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#returns a float (larger in well separated clusters)\n",
    "\n",
    "#Silhouette score (ratio between the cohesiveness of a cluster and its separation from other clusters)\n",
    "simple_sil_score = si.simplified_silhouette_score(all_pcs=all_pcs, all_labels=all_labels, this_unit_id=0)\n",
    "#Close to 1 = good clustering. Close to -1 = poorly isolated cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward way to filter a pandas dataframe is via the `query`.\n",
    "We first define our query (make sure the names match the column names of the dataframe):\n",
    "and then we can use the query to select units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Automatic curation based on parameters\n",
    "isi_viol_thresh = 0.5\n",
    "amp_cutoff_thresh = 0.1\n",
    "\n",
    "our_query = f\"amplitude_cutoff < {amp_cutoff_thresh} & isi_violations_ratio < {isi_viol_thresh}\"\n",
    "print(our_query)\n",
    "\n",
    "keep_units = df.query(our_query)\n",
    "keep_unit_ids = keep_units.index.values\n",
    "keep_unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_auto_KS4 = sorting_KS4.select_units(keep_unit_ids)\n",
    "print(f\"Number of units before curation: {len(sorting_KS4.get_unit_ids())}\")\n",
    "print(f\"Number of units after curation: {len(sorting_auto_KS4.get_unit_ids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_curated = sa.select_units(keep_unit_ids)\n",
    "\n",
    "#Saving Analyzer in specific format and loading it from saved\n",
    "sa_curated_saved = sa_curated.save_as(format=\"zarr\", folder=data_folder / \"sorting_analyzer_curated.zarr\")\n",
    "print(sa_curated_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Viewers <a class=\"anchor\" id=\"viewers\"></a>\n",
    "### SpikeInterface GUI\n",
    "Can be run directly in a terminal with: \n",
    "sigui /path/to/analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%gui qt\n",
    "si.plot_sorting_summary(sorting_analyzer=sa, curation=True, backend='spikeinterface_gui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load times from timestamps csv files\n",
    "tms_files = sorted(glob.glob(os.path.join(data_folder, \"Timestamps*.csv\")))\n",
    "concatenated_first_last = pd.DataFrame()\n",
    "for tms_file in tms_files:\n",
    "    df = pd.read_csv(tms_file, header=None, names=['Timestamps'])#(usecols=[0], nrows=1)\n",
    "    df['Timestamps'] = df['Timestamps'].str.slice(0, 15)\n",
    "    first = df.head(1)\n",
    "    last = df.tail(1)\n",
    "    concatenated_first_last = pd.concat([concatenated_first_last, first, last], ignore_index=True)\n",
    "#print(concatenated_first_last)\n",
    "\n",
    "# Step 2: Process TTLs from csv files\n",
    "ttl_files = sorted(glob.glob(os.path.join(data_folder, \"TTL*.csv\")))\n",
    "concatenated_df_ttl = pd.DataFrame()\n",
    "for ttl_file in ttl_files:\n",
    "    df = pd.read_csv(ttl_file, header=None, names=['Timestamps'])#(usecols=[0], nrows=1)\n",
    "    df['Timestamps'] = df['Timestamps'].str.slice(0, 15)\n",
    "    concatenated_df_ttl = pd.concat([concatenated_df_ttl, df], ignore_index=True)\n",
    "#print(concatenated_df_ttl)\n",
    "\n",
    "# Step 3: Process Events from csv files\n",
    "events_files = sorted(glob.glob(os.path.join(data_folder, \"Events*.csv\")))\n",
    "concatenated_df_events = pd.DataFrame()\n",
    "for event_file in events_files:\n",
    "    df = pd.read_csv(event_file, usecols=[0, 1], header=None, names=['Timestamps_Start', 'Timestamps_Stop'])\n",
    "    df['Timestamps_Start'] = df['Timestamps_Start'].str.slice(0, 15)\n",
    "    df['Timestamps_Stop'] = df['Timestamps_Stop'].str.slice(0, 15)\n",
    "    concatenated_df_events = pd.concat([concatenated_df_events, df], ignore_index=True)\n",
    "#print(concatenated_df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = si.plot_unit_templates(sa, backend=\"ipywidgets\")#templ\n",
    "waveforms = si.plot_unit_waveforms(sa, backend=\"ipywidgets\")#wf\n",
    "unit_locations = si.plot_unit_locations(sa, backend=\"ipywidgets\")#u_loc\n",
    "spk_locations = si.plot_spike_locations(sa, backend=\"ipywidgets\")#spk_loc\n",
    "spk_amplitude = si.plot_amplitudes(sa, backend=\"ipywidgets\")#spk_amp\n",
    "template_similarity = si.plot_template_similarity(sa)#, backend=\"ipywidgets\")#templ_sim\n",
    "autocorr = si.plot_autocorrelograms(sa, unit_ids=sorting_KS4.unit_ids[::10])#, backend=\"ipywidgets\")#corr\n",
    "crosscorr = si.plot_crosscorrelograms(sa, unit_ids=sorting_KS4.unit_ids[::10])#, backend=\"ipywidgets\")#corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortingView\n",
    "Web-based, shareable (with link), sorter visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = si.plot_sorting_summary(sa_curated, curation=True, backend='sortingview')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 9. Exporters <a class=\"anchor\" id=\"exporters\"></a>\n",
    "#### Export to Phy for manual curation [Phy](https://github.com/cortex-lab/phy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_to_phy(sa, output_folder=data_folder / 'phy_KS4_RN', compute_pc_features=True,\n",
    "                   copy_binary=True, dtype='float32', compute_amplitudes=True,\n",
    "                   sparsity=sparsity, add_quality_metrics=True, add_template_metrics=True, \n",
    "                   template_mode='median', verbose=True,**job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "!phy template-gui phy_KS4_RN/params.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After curating the results we can reload it using the `PhySortingExtractor` and exclude the units that we labeled as `noise`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_phy_curated = si.read_phy(data_folder / 'phy_KS4_RN/', exclude_cluster_groups=['noise'])\n",
    "print(f\"Number of units before curation: {len(sorting_KS4.get_unit_ids())}\")\n",
    "print(f\"Number of units after curation: {len(sorting_phy_curated.get_unit_ids())}\")\n",
    "#Save the loaded curated phy into Spikeinterface object!!\n",
    "#si.export_report(sa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SI_env2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
